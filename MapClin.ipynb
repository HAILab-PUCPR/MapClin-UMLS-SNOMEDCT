{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install click==7.0\n",
    "#!pip install Exit\n",
    "#!pip3 install --upgrade gensim --user\n",
    "#!pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
    "##import sys\n",
    "##!{sys.executable} -m conda install -c conda-forge spacy-model-pt_core_news_sm\n",
    "#!pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "import unidecode\n",
    "import nltk\n",
    "import re\n",
    "import unicodedata\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "#!pip install snowballstemmer #import realizado em outro lugar, necessario instalar\n",
    "#!pip install stanza\n",
    "#stanza.download('pt')\n",
    "import stanza\n",
    "from dicio import Dicio\n",
    "#https://github.com/felipemfp/dicio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import collections\n",
    "from collections import deque\n",
    "from operator import itemgetter\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Functions and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def purge_dublicates(X):\n",
    "    unique_X = []\n",
    "    for i, row in enumerate(X):\n",
    "        if row not in X[i + 1:]:\n",
    "            unique_X.append(row)\n",
    "    return unique_X\n",
    "\n",
    "def levenshtein(source, target):    \n",
    "    if len(source) < len(target):\n",
    "        return levenshtein(target, source)\n",
    "\n",
    "    # So now we have len(source) >= len(target).\n",
    "    if len(target) == 0:\n",
    "        return len(source)\n",
    "\n",
    "    # We call tuple() to force strings to be used as sequences\n",
    "    # ('c', 'a', 't', 's') - numpy uses them as values by default.\n",
    "    source = np.array(tuple(source))\n",
    "    target = np.array(tuple(target))\n",
    "\n",
    "    # We use a dynamic programming algorithm, but with the\n",
    "    # added optimization that we only need the last two rows\n",
    "    # of the matrix.\n",
    "    previous_row = np.arange(target.size + 1)\n",
    "    for s in source:\n",
    "        # Insertion (target grows longer than source):\n",
    "        current_row = previous_row + 1\n",
    "\n",
    "        # Substitution or matching:\n",
    "        # Target and source items are aligned, and either\n",
    "        # are different (cost of 1), or are the same (cost of 0).\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                np.add(previous_row[:-1], target != s))\n",
    "\n",
    "        # Deletion (target grows shorter than source):\n",
    "        current_row[1:] = np.minimum(\n",
    "                current_row[1:],\n",
    "                current_row[0:-1] + 1)\n",
    "\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "\n",
    "\n",
    "class FileManager:\n",
    "    def open_xlsx_folder(self, folder_path):\n",
    "        '''\n",
    "        Leitura de arquivos xlsx (le todos os arquivos em uma pasta)\n",
    "        Necessário fornecer o caminho da pasta\n",
    "        '''\n",
    "        paths = glob.glob(folder_path +\"/*.xlsx\")\n",
    "        data = pd.DataFrame()   \n",
    "        for path in paths:\n",
    "            df = pd.read_excel(path)\n",
    "            print('Path: ',path,'\\nLen: ',len(df),'\\n')\n",
    "            data = data.append(df)\n",
    "        data.reset_index(inplace = True)\n",
    "        return data\n",
    "    \n",
    "    def open_xlsx(self, path):\n",
    "        df = pd.read_excel(path)\n",
    "    \n",
    "        return df\n",
    "    \n",
    "    def open_txt(self, path, header):\n",
    "        if header == True:\n",
    "            df = pd.read_csv(path, delimiter = '\\n')\n",
    "        else:\n",
    "            df = pd.read_csv(path, delimiter = '\\n', header = None)\n",
    "    \n",
    "        return df\n",
    "\n",
    "class WordEmbeddingClass:\n",
    "    __instance = None\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def getInstance(cls,WEpath):\n",
    "        if not cls.__instance:\n",
    "            cls.__instance = WordEmbeddingClass()\n",
    "            try:\n",
    "                cls.__instance.model = KeyedVectors.load_word2vec_format(WEpath, binary=True)\n",
    "            except:\n",
    "                try:\n",
    "                    cls.__instance.model = KeyedVectors.load_word2vec_format(WEpath)\n",
    "                except:\n",
    "                    cls.__instance.model = Word2Vec.load(WEpath)\n",
    "            #print(\"Word Embedding com:\",len(cls.__instance.model.vocab),\"Tokens.\")\n",
    "        return cls.__instance\n",
    "\n",
    "    @classmethod    \n",
    "    def deleteInstance(cls):\n",
    "        cls.__instance = None\n",
    "    \n",
    "    def getSimilar(self,termo,topn=10):\n",
    "        try:\n",
    "            return self.model.most_similar(termo, topn=topn)\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def getSimilarity(self,termo1,termo2):\n",
    "        try:\n",
    "            dist = self.model.wmdistance(termo1, termo2)\n",
    "            if dist == float('inf'):\n",
    "                return 0.0\n",
    "            return dist\n",
    "        except:\n",
    "            return 0.0        \n",
    "        \n",
    "#WEC = WordEmbeddingClass.getInstance('corpus_incor.bin')\n",
    "#print(WEC.getSimilar(['referir']))\n",
    "#print(\"Similaridade:\",WEC.getSimilarity(['referir'], ['dor']))\n",
    "#WEC.deleteInstance()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PreProcessamento:\n",
    "    __instance = None\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def getInstance(cls):\n",
    "        if not cls.__instance:\n",
    "            cls.__instance = PreProcessamento()\n",
    "            cls.__instance.lemmatizer = nltk.WordNetLemmatizer()\n",
    "            cls.__instance.nlp = stanza.Pipeline('pt', processors='tokenize,mwt,pos,lemma', tokenize_pretokenized=True)\n",
    "        return cls.__instance\n",
    "        \n",
    "    def separaTextoEmSentenças(self,texto): # NÂO tokenizado\n",
    "        sent_tokenizer = nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
    "        return sent_tokenizer.tokenize(texto)\n",
    "    \n",
    "    def tokeniza(self,sentenca): # NÂO tokenizado\n",
    "        #return self.tokenizer.tokenize(texto)\n",
    "        return nltk.tokenize.word_tokenize(sentenca, language='portuguese')\n",
    "    \n",
    "    def expandeAcronimo(self,texto): #tokenizado\n",
    "        #quanto acronimo esta em minusculo ele não é expandido, \n",
    "        #porém colocar tudo para upper case faz com que não acronimos sejam expandidos, levando a erros! (como \"da\")\n",
    "        import pickle\n",
    "                \n",
    "        stopWordsList = ['de', 'do', 'dos', 'da', 'das', 'uma', 'quem', 'por', 'o', 'as', 'a', 'as', 'com', 'para', 'seu', 'uns', 'umas', 'e', 'ou', 'pra', 'na', 'nas', 'no', 'pelo', 'como', 'sua', 'nos', 'ao', 'aos', 'em', 'que', 'um', 'pela']\n",
    "        acronimosDict = pickle.load(open(\"DefaultAcronymsNew.pkl\", \"rb\"))\n",
    "        newTexto = []\n",
    "        for token in texto:\n",
    "            if (token.lower() in acronimosDict or token.upper() in acronimosDict) and token.lower() not in stopWordsList:\n",
    "                expandido = self.tokeniza(acronimosDict[token])\n",
    "                newTexto.extend(expandido)\n",
    "            else:\n",
    "                newTexto.append(token)\n",
    "        return newTexto\n",
    "    \n",
    "    def expandeAcronimoCompleto(self,texto): #tokenizado\n",
    "        import pickle\n",
    "                \n",
    "        stopWordsList = ['de', 'do', 'dos', 'da', 'das', 'uma', 'quem', 'por', 'o', 'as', 'a', 'as', 'com', 'para', 'seu', 'uns', 'umas', 'e', 'ou', 'pra', 'na', 'nas', 'no', 'pelo', 'como', 'sua', 'nos', 'ao', 'aos', 'em', 'que', 'um', 'pela']\n",
    "        #acronimosDict = pickle.load(open(\"DefaultAcronymsNew.pkl\", \"rb\"))\n",
    "        acronimosList = pickle.load(open(\"DefaultAcronymsNewList.pkl\", \"rb\"))\n",
    "        \n",
    "        acronyms,expansions = list(zip(*acronimosList))\n",
    "        \n",
    "        newTexto = []\n",
    "        for token in texto:\n",
    "            tokenLower = token.lower()\n",
    "            if tokenLower in stopWordsList:\n",
    "                newTexto.append(token)\n",
    "            else:\n",
    "                #if token in acronimosDict and token.lower() not in stopWordsList:\n",
    "                encontrou = False\n",
    "                for i in range(len(acronyms)):\n",
    "                    acronym = acronyms[i]\n",
    "                    if token.lower() == acronym or token.upper() == acronym:\n",
    "                        expandido = self.tokeniza(expansions[i])\n",
    "                        #newTexto.extend(expandido)\n",
    "                        newTexto.append(expandido)\n",
    "                        encontrou = True\n",
    "                if not encontrou:\n",
    "                    newTexto.append(token)\n",
    "        return newTexto\n",
    "    \n",
    "    def removeALLspace(self,string): # NÂO tokenizado\n",
    "        ns = \"\"\n",
    "        for s in string:\n",
    "            if s != \" \":\n",
    "                ns+=s\n",
    "        return ns\n",
    "    \n",
    "    def normalizaAcento(self,text): # NÂO tokenizado    \n",
    "        try:\n",
    "            text = unicode(text, 'utf-8')\n",
    "        except NameError: # unicode is a default on python 3 \n",
    "            pass\n",
    "    \n",
    "        text = unicodedata.normalize('NFD', text)\\\n",
    "               .encode('ascii', 'ignore')\\\n",
    "               .decode(\"utf-8\")\n",
    "\n",
    "        novo_s = re.sub(\"Ã…|Ã€|Ã |Ãƒ|Ã‚|Ã„\",\"A\",text);\n",
    "        novo_s=re.sub(\"Ã¥|Ã |Ã¡|Ã£|Ã¢|Ã¤\",\"a\",novo_s);\n",
    "        novo_s=re.sub(\"Ã¨|Ã©|Ã«|Ãª\",\"e\",novo_s);\n",
    "        novo_s=re.sub(\"Ãˆ|Ã‰|Ã‹|ÃŠ\",\"E\",novo_s);\n",
    "        novo_s=re.sub(\"Ã¬|Ã­|Ã¯|Ã®\",\"i\",novo_s);\n",
    "        novo_s=re.sub(\"ÃŒ|Ã |Ã |ÃŽ]\",\"I\",novo_s);\n",
    "        novo_s=re.sub(\"Ã²|Ã³|Ã¶|Ã´|Ãµ\",\"o\",novo_s);\n",
    "        novo_s=re.sub(\"Ã'|Ã\\\"|Ã-|Ã•\",\"O\",novo_s);\n",
    "        novo_s=re.sub(\"Ã¹|Ãº|Ã¼|Ã»/\",\"u\",novo_s);\n",
    "        novo_s=re.sub(\"Ã™|Ãš|Ãœ|Ã›/\",\"U\",novo_s);\n",
    "        novo_s=re.sub(\"Ã½|Ã¿\",\"y\",novo_s);\n",
    "        novo_s=re.sub(\"Ã |Å¸\",\"Y\",novo_s);\n",
    "        novo_s=re.sub(\"Ã§\",\"c\",novo_s);\n",
    "        novo_s=re.sub(\"Ã‡\",\"C\",novo_s);\n",
    "        \n",
    "        return str(text)\n",
    "    \n",
    "    def normalizaAcentoERemoveCaracterEspecial(self,texto): #tokenizado\n",
    "        newTexto = []\n",
    "        for token in texto:\n",
    "            newToken = ''.join(e for e in unidecode.unidecode(token) if e.isalnum())\n",
    "            if len(newToken) != 0:\n",
    "                newTexto.append(newToken)\n",
    "        return newTexto        \n",
    "        \n",
    "    def removeCaracteresNaoAlfabeticos(self,string): # NÃO tokenizado\n",
    "        return re.sub(\"[^A-Za-z']+\", ' ', string)\n",
    "    \n",
    "    def removeCaracteresNaoAlfabeticos(self,texto): # tokenizado\n",
    "        # remove punctuations\n",
    "        remove = string.punctuation \n",
    "        texto = [''.join(c for c in s if c not in remove) for s in texto]\n",
    "        return texto\n",
    "        \n",
    "    def removeEspacoVazioAntesDepoisSTR(self, string): #NÃO tokenizado\n",
    "        return string.strip()\n",
    "        \n",
    "    def removeEspacoVazioAntesDepoisTOKENIZED(self, texto): #tokenizado\n",
    "        newTexto = []\n",
    "        for token in texto:\n",
    "            newTexto.append(self.removeEspacoVazioAntesDepoisSTR(token))\n",
    "        return newTexto    \n",
    "    \n",
    "    def normalizaPontuacao(self, texto:str) -> str: #NÃO tokenizado\n",
    "        '''Makes basic punctuation normalizations.'''\n",
    "\n",
    "        #normalize punctuation\n",
    "        texto=re.sub('>=',\" maior igual a \",texto)\n",
    "        texto=re.sub('<=',\" menor igual a \",texto)\n",
    "        texto=re.sub(';',' ; ',texto)\n",
    "        texto=re.sub('>>','~~~',texto)\n",
    "        texto=re.sub('<<','°°°',texto)\n",
    "        texto=re.sub('>',' maior que ',texto)\n",
    "        texto=re.sub('<',' menor que ',texto)\n",
    "        texto=re.sub('~~~','>>',texto)\n",
    "        texto=re.sub('°°°','<<',texto)\n",
    "        texto=re.sub('=', ' = ',texto)\n",
    "        texto=re.sub('\\:',' : ',texto)\n",
    "        texto=re.sub('\\[', ' [ ', texto)\n",
    "        texto=re.sub('\\]', ' ] ',texto)\n",
    "        texto=re.sub('\\(', ' ( ',texto)\n",
    "        texto=re.sub('\\)', ' ) ',texto)\n",
    "        return texto\n",
    "           \n",
    "    def normalizaMedidas(self, text:str) -> str: #NÃO tokenizado\n",
    "        '''Makes basic number and physics grandeur normalization.'''\n",
    "        \n",
    "        #protect the decimal's '.' from the period normalization\n",
    "        times = re.findall('\\d+x\\/\\w+',text)\n",
    "        for time in times:\n",
    "            t = re.search('(\\d+x)\\/(\\w+)', time)\n",
    "            text = re.sub(t.group(1) + '\\/' + t.group(2), t.group(1) + ' por ' + t.group(2), text)\n",
    "\n",
    "        #protect the decimal's '.' from the period normalization\n",
    "        decimals = re.findall('\\d+\\.\\d+',text)\n",
    "        for decimal in decimals:\n",
    "            d = re.search('(\\d+)\\.(\\d+)', decimal)\n",
    "            text = re.sub(d.group(1) + '\\.' + d.group(2), d.group(1) + 'SINALDECIMAL' + d.group(2), text)\n",
    "\n",
    "        #protect the decimal's ',' from the comma normalization\n",
    "        decimals = re.findall('\\d+\\,\\d+',text)\n",
    "        for decimal in decimals:\n",
    "            d = re.search('(\\d+)\\,(\\d+)', decimal)\n",
    "            text = re.sub(d.group(1) + '\\,' + d.group(2), d.group(1) + 'SINALVIRGULA' + d.group(2), text)\n",
    "\n",
    "        #normalize gaps\n",
    "        gaps=re.findall('\\d-\\d',text)\n",
    "        for gap in gaps:\n",
    "            g=re.search('(\\d)-(\\d)',gap)\n",
    "            text=re.sub(g.group(1)+'-'+g.group(2),g.group(1)+' - '+g.group(2),text)\n",
    "\n",
    "        #normalize periods\n",
    "        periods = re.findall('\\w\\.\\s?',text)\n",
    "        for period in periods:\n",
    "            p=re.search('(\\w)\\.\\s?',period)\n",
    "            text=re.sub(p.group(1)+'\\.',p.group(1)+' . ',text)\n",
    "\n",
    "        #normalize commas\n",
    "        text=re.sub(',',' , ',text)\n",
    "\n",
    "        #return the decimal's '.'\n",
    "        text=re.sub('SINALDECIMAL','.',text)\n",
    "        text=re.sub('SINALVIRGULA',',',text)\n",
    "        \n",
    "        word2trig =    {'milimetros':'mm','milímetros':'mm','milímetro':'mm','milimetro':'mm','centimetros':'cm','centímetros':'cm','centímetro':'cm','centimetro':'cm','decimetros':'dc','decímetros':'dc','decímetro':'dc','decimetro':'dc','metros':'m','metro':'m','decametros':'dam','decâmetros':'dam','decametro':'dam','decâmetro':'dam','quilometros':'km','quilômetros':'km','quilômetro':'km','quilometro':'km','milisegundos':'ms','milisegundo':'ms','segundos':'s','segundo':'s','minutos':'min','minuto':'min','horas':'h','hora':'h','miligramas':'mg','miligrama':'mg','gramas':'g','grama':'g','quilogramas':'kg','quilos':'kg','quilo':'kg','quilograma':'kg','mililitros':'ml','mililitro':'ml','litros':'l','litro':'l'}\n",
    "        \n",
    "        #normalize physics grandeurs\n",
    "        for word in word2trig:\n",
    "            change = word2trig[word]\n",
    "            pattern = f'(\\s{word}\\s)'\n",
    "            text = re.sub(pattern, ' '+change+' ', text)\n",
    "        \n",
    "        word2number =  {'um':'1','dois':'2','duas':'2','três':'3','quatro':'4','cinco':'5','seis':'6','sete':'7','oito':'8','nove':'9','dez':'10','onze':'11','doze':'12','treze':'13','quatorze':'14','quinze':'15','dezesseis':'16','dezessete':'17','dezoito':'18','dezenove':'19','vinte':'20','trinta':'30','quarenta':'40','cinquenta':'50','sessenta':'60','setenta':'70','oitenta':'80','noventa':'90','cem':'100','cento':'100','duzentos':'200','trezentos':'300','quatrocentos':'400','quinhentos':'500','seiscentos':'600','setecentos':'700','oitocentos':'800','novecentos':'900','mil':'1000','milhão':'1000000'}\n",
    "        \n",
    "        triggers = ['mm', 'cm', 'dm', 'm', 'dam', 'hm', 'km', 'ms', 's',\n",
    "                    'min', 'h', 'hs', 'mg', 'ng', 'g', 'kg', 'ml', 'l',\n",
    "                    'anos', 'ano', 'mês', 'mes', 'meses', 'dia', 'dias',\n",
    "                    'mol', 'nmol', 'mmol', 'umol']\n",
    "        \n",
    "        #normalize numbers\n",
    "        for word in word2number:\n",
    "            change = word2number[word]\n",
    "            if word != 'um':\n",
    "                pattern = f'(\\s{word}\\s)'\n",
    "                text = re.sub(pattern, ' '+change+' ', text)\n",
    "            else:\n",
    "                for t in triggers:\n",
    "                    pattern = f'(\\s{word}\\s(?={t}\\s))'\n",
    "                    text = re.sub(pattern, ' '+change+' ', text)  \n",
    "        \n",
    "        return text\n",
    "        \n",
    "    def normalizaMinusculo(self,texto): #tokenizado\n",
    "        newTexto = []\n",
    "        for token in texto:\n",
    "            newTexto.append(token.lower())\n",
    "        return newTexto\n",
    "    \n",
    "    def removeStopWords1(self,texto): #tokenizado\n",
    "        #usado por Lucas Ronnau\n",
    "        stopWordsList = ['de', 'do', 'dos', 'da', 'das', 'uma', 'quem', 'por', 'o', 'as', 'a', 'as', 'com', 'para', 'seu', 'uns', 'umas', 'e', 'ou', 'pra', 'na', 'nas', 'no', 'pelo', 'como', 'sua', 'nos', 'ao', 'aos', 'em', 'que', 'um', 'pela']\n",
    "        newTexto = []\n",
    "        for token in texto:\n",
    "            if token.lower() not in stopWordsList:\n",
    "                newTexto.append(token)\n",
    "        return newTexto\n",
    "    \n",
    "    def removeStopWords2(self,texto): #tokenizado\n",
    "        # stop words do nltk\n",
    "        stopWordsList = set(stopwords.words('portuguese'))\n",
    "        stopWordsList.remove('não')\n",
    "        newTexto = []\n",
    "        for token in texto:\n",
    "            if token not in self.stopWordsList:\n",
    "                newTexto.append(token)\n",
    "        return newTexto\n",
    "\n",
    "    def destokeniza(self,texto):\n",
    "        a = \"\"\n",
    "        for token in texto:\n",
    "            a += \" \" + token.replace(\" \", \"-\")\n",
    "        return a[1:]\n",
    "    \n",
    "    def spacyPipeNormalization(self,textos): #mais que um texto #alterado de Giovanni\n",
    "        import spacy #será chamado uma vez, então tudo bem importar aqui\n",
    "        nlp = spacy.load('pt_core_news_sm', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "        newTexts = []\n",
    "        for doc in nlp.pipe(texto, batch_size=5000, n_threads=-1):\n",
    "            newTexts.append([token.lemma_ for token in doc if not token.is_stop])\n",
    "        return newTexts\n",
    "    \n",
    "    def stemmer(self,text):  # NÃO tokenizado\n",
    "        import snowballstemmer\n",
    "        stemmer = snowballstemmer.stemmer('portuguese')\n",
    "        return ' '.join(stemmer.stemWords(text.split())).strip()\n",
    "\n",
    "    def lematizador(self,text):  # tokenizado\n",
    "        lemma = []\n",
    "        try:\n",
    "            for sent in self.nlp([text]).sentences:\n",
    "                for word in sent.words:\n",
    "                    lemma.append(word.lemma)\n",
    "        except:\n",
    "            pass\n",
    "        return lemma\n",
    "    \n",
    "    def processa(self,texto):\n",
    "        #texto = self.removeALLspace(texto)\n",
    "        #texto = self.removeCaracteresNaoAlfabeticos(texto)\n",
    "        texto = self.normalizaAcento(texto)\n",
    "        texto = self.normalizaPontuacao(texto)\n",
    "        texto = self.normalizaMedidas(texto)\n",
    "            \n",
    "        texto = self.tokeniza(texto)\n",
    "\n",
    "        texto = self.removeEspacoVazioAntesDepoisTOKENIZED(texto)\n",
    "        texto = self.expandeAcronimo(texto)\n",
    "        texto = self.normalizaAcentoERemoveCaracterEspecial(texto) #função de normalizar acento mais simples que \"removeAcento\"\n",
    "        texto = self.normalizaMinusculo(texto)\n",
    "        texto = self.removeStopWords2(texto)\n",
    "        #texto = self.lematizador(texto)\n",
    "        \n",
    "        texto = self.destokeniza(texto)\n",
    "        \n",
    "        texto = self.stemmer(texto)\n",
    "            \n",
    "        texto = self.tokeniza(texto)\n",
    "\n",
    "        return texto\n",
    "               \n",
    "    def normalizaMAPCLIN(self,texto):\n",
    "        texto = self.normalizaAcentoERemoveCaracterEspecial(texto)\n",
    "        texto = self.normalizaMinusculo(texto)\n",
    "        texto = self.removeStopWords1(texto)\n",
    "        return texto\n",
    "    \n",
    "    def createMultipleListOfAcronyms(self,texto,current):\n",
    "        newTexto = []\n",
    "        for i in range(len(texto)):\n",
    "            token = texto[i]\n",
    "            if type(token) == str:\n",
    "                newTexto.append(token)\n",
    "            elif i == current:\n",
    "                newTexto.extend(token)\n",
    "        return newTexto\n",
    "    \n",
    "    def processaMAPCLINbusca(self,texto):\n",
    "        if type(texto) == str:\n",
    "            texto = self.tokeniza(texto)\n",
    "        texto = self.expandeAcronimoCompleto(texto)\n",
    "        textos = []\n",
    "        for j in range(len(texto)):\n",
    "            textos.append(self.createMultipleListOfAcronyms(texto,j))\n",
    "        newTextos = []\n",
    "        for texto in textos:\n",
    "            newTextos.append(self.normalizaMAPCLIN(texto))\n",
    "        return purge_dublicates(newTextos)\n",
    "    \n",
    "    def processaMAPCLIN(self,texto):\n",
    "        if type(texto) == str:\n",
    "            texto = self.tokeniza(texto)\n",
    "        texto = self.expandeAcronimo(texto)\n",
    "        texto = self.normalizaMAPCLIN(texto)\n",
    "        return newTextos\n",
    "    \n",
    "\n",
    "#PreProcessamento.getInstance().processa(\"éstAvÃ§ andava na UTI Ontem? (2 > quatro miligrama!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening and processing data from UMLS and creating intermediate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class UMLSclass:\n",
    "    def __init__(self,preProcessamentoclass):\n",
    "        self.preProcessamento = preProcessamentoclass\n",
    "    \n",
    "    def selectUMLS(self,dadosUMLS):\n",
    "        MRCONSO = dadosUMLS[0]\n",
    "        MRXW_POR = dadosUMLS[1]\n",
    "        MRREL = dadosUMLS[2]\n",
    "        \n",
    "        newMRCONSO = []\n",
    "        for linha in MRCONSO:\n",
    "            #pega coluna com termos\n",
    "            posicao = 2 #PRECISA PEGAR POSICAO\n",
    "            termo = linha[posicao]\n",
    "            termoProcessado = self.preProcessamento.processaMAPCLIN(termo)\n",
    "            termoSTR = self.preProcessamento.destokeniza(termoProcessado)\n",
    "            newMRCONSO.append((linha[0],linha[1],termoProcessado,termoSTR,termo))\n",
    "            \n",
    "        newMRXW_POR = []\n",
    "        for linha in MRXW_POR:\n",
    "            #pega coluna com termos\n",
    "            posicao = 2 #PRECISA PEGAR POSICAO\n",
    "            termo = linha[posicao]\n",
    "            termoProcessado = self.preProcessamento.processaMAPCLIN(termo)            \n",
    "            termoSTR = self.preProcessamento.destokeniza(termoProcessado)\n",
    "            newMRXW_POR.append((linha[0],linha[1],termoProcessado,termoSTR,termo))\n",
    "            \n",
    "        return newMRCONSO,newMRXW_POR,MRREL #retornar somente as listas de termos\n",
    "    \n",
    "    def stemmUMLS(self,UMLSnormalizada):\n",
    "        #add stem column\n",
    "        MRCONSO = UMLSnormalizada[0]\n",
    "        MRXW_POR = UMLSnormalizada[1]\n",
    "        MRREL = UMLSnormalizada[2]\n",
    "        \n",
    "        newMRCONSO = []\n",
    "        for linha in MRCONSO:\n",
    "            #pega coluna com termos\n",
    "            posicao = 2 #PRECISA PEGAR POSICAO\n",
    "            termo = linha[posicao]\n",
    "            termo = self.preProcessamento.destokeniza(termo)\n",
    "            termoSTEMM = self.preProcessamento.stemmer(termo) #precisa ser string\n",
    "            #linha[posicao] = termoProcessado\n",
    "            newMRCONSO.append((linha[0],linha[1],linha[2],linha[3],linha[4],termoSTEMM))\n",
    "            \n",
    "        newMRXW_POR = []\n",
    "        for linha in MRXW_POR:\n",
    "            #pega coluna com termos\n",
    "            posicao = 2 #PRECISA PEGAR POSICAO\n",
    "            termo = linha[posicao]\n",
    "            termo = self.preProcessamento.destokeniza(termo)\n",
    "            termoSTEMM = self.preProcessamento.stemmer(termo) #precisa ser string\n",
    "            #linha[posicao] = termoProcessado\n",
    "            newMRXW_POR.append((linha[0],linha[1],linha[2],linha[3],linha[4],termoSTEMM))\n",
    "            \n",
    "        return newMRCONSO,newMRXW_POR,MRREL #retornar somente as listas de termos        \n",
    "        \n",
    "    def lematizaUMLS(self,UMLSnormalizada):\n",
    "        #add lemma column\n",
    "        MRCONSO = UMLSnormalizada[0]\n",
    "        MRXW_POR = UMLSnormalizada[1]\n",
    "        MRREL = UMLSnormalizada[2]\n",
    "        \n",
    "        newMRCONSO = []\n",
    "        ci = 0\n",
    "        print(len(MRCONSO)+len(MRXW_POR))\n",
    "        for linha in MRCONSO:\n",
    "            print(ci,end=\"\\r\")\n",
    "            ci=ci+1\n",
    "            #pega coluna com termos\n",
    "            posicao = 2 #PRECISA PEGAR POSICAO\n",
    "            termo = linha[posicao]\n",
    "            termoLEMMA = self.preProcessamento.lematizador(termo) \n",
    "            #linha[posicao] = termoProcessado\n",
    "            newMRCONSO.append((linha[0],linha[1],linha[2],linha[3],linha[4],linha[5],termoLEMMA))\n",
    "            \n",
    "        newMRXW_POR = []\n",
    "        for linha in MRXW_POR:\n",
    "            print(ci,end=\"\\r\")\n",
    "            ci=ci+1\n",
    "            #pega coluna com termos\n",
    "            posicao = 2 #PRECISA PEGAR POSICAO\n",
    "            termo = linha[posicao]\n",
    "            termoLEMMA = self.preProcessamento.lematizador(termo) \n",
    "            #linha[posicao] = termoProcessado\n",
    "            newMRXW_POR.append((linha[0],linha[1],linha[2],linha[3],linha[4],linha[5],termoLEMMA))\n",
    "            \n",
    "        return newMRCONSO,newMRXW_POR,MRREL #retornar somente as listas de termos        \n",
    "        \n",
    "    \n",
    "    def openFiles(self,files):\n",
    "        MRCONSOFILE = files[0]\n",
    "        MRXW_PORFILE = files[1]\n",
    "        MRRELFILE = files[2]\n",
    "        \n",
    "        ############################################\n",
    "\n",
    "        f = open(MRXW_PORFILE, \"r\")\n",
    "        MRXW_PORRAW = f.read().split(\"\\n\")\n",
    "        del MRXW_PORRAW[-1] #ultima linha é vazia\n",
    "        f.close()\n",
    "        ##print(len(MRXW_PORRAW)) #1589110\n",
    "        ##print(MRXW_PORRAW[123400]) #'POR|amostra|C0550117|L12632755|S15615562|'\n",
    "        \n",
    "        MRXW_POR = []\n",
    "        for y,linha in enumerate(MRXW_PORRAW):\n",
    "            linhaSeparada =  linha.split(\"|\")\n",
    "            CUI = linhaSeparada[2]\n",
    "            WD  = linhaSeparada[1]\n",
    "            MRXW_POR.append((y,CUI,WD)) #cui, word in lower case\n",
    "\n",
    "        ############################################\n",
    "\n",
    "        with open(MRCONSOFILE, \"r\", encoding=\"utf8\") as MRCONSORAW:\n",
    "            # C0003803|SPA|S|L3401341|PF|S3928999|Y|A9197782||M0001704|D001139|\n",
    "            # MSHSPA|MH|D001139|Malformación de Arnold-Chiari|3|N||\n",
    "            # len = 425742\n",
    "            MRCONSO = []\n",
    "            for y,linha in enumerate(MRCONSORAW):\n",
    "                linhaSeparada =  linha.split(\"|\")\n",
    "                CUI = linhaSeparada[0]\n",
    "                STR  = linhaSeparada[14]\n",
    "                LAT  = linhaSeparada[1]\n",
    "                if LAT == \"POR\":\n",
    "                    MRCONSO.append((y,CUI,STR))\n",
    "                    \n",
    "        ############################################\n",
    "\n",
    "        with open(MRRELFILE, \"r\", encoding=\"utf8\") as MRRELRAW:\n",
    "            # C0851745|A31370316|SDUI|CHD|C0003466|A31264097|SDUI||R191307594||MDRKOR||||N||\n",
    "    \n",
    "            MRREL = []\n",
    "            for y,linha in enumerate(MRRELRAW):\n",
    "                linhaSeparada =  linha.split(\"|\")\n",
    "                CUI1 = linhaSeparada[0]\n",
    "                CUI2  = linhaSeparada[4]\n",
    "                REL = linhaSeparada[3]\n",
    "                if REL == \"SY\" and CUI1 != CUI2:\n",
    "                    MRREL.append((y,CUI1,CUI2))\n",
    "                    \n",
    "        \n",
    "        return MRCONSO,MRXW_POR,MRREL\n",
    "    \n",
    "    def openFilesUMLStoSNOMED(self,MRCONSOFILE,MRRELFILE):\n",
    "    \n",
    "        with open(MRCONSOFILE, \"r\", encoding=\"utf8\") as MRCONSORAW:\n",
    "            # C0003803|SPA|S|L3401341|PF|S3928999|Y|A9197782||M0001704|D001139|\n",
    "            # MSHSPA|MH|D001139|Malformación de Arnold-Chiari|3|N||\n",
    "            # len = 425742\n",
    "            snomed = []\n",
    "            for y,linha in enumerate(MRCONSORAW):\n",
    "                linhaSeparada =  linha.split(\"|\")\n",
    "                CUI  = linhaSeparada[0]\n",
    "                STT  = linhaSeparada[4]\n",
    "                ISPREF = linhaSeparada[6]\n",
    "                SAB  = linhaSeparada[11]\n",
    "                TTY  = linhaSeparada[12]\n",
    "                CODE = linhaSeparada[13]          \n",
    "                STR  = linhaSeparada[14]\n",
    "                if SAB == \"SNOMEDCT_US\":\n",
    "                    snomed.append((y,CODE,STR,CUI,TTY,ISPREF,STT))\n",
    "                    \n",
    "        ############################################\n",
    "    \n",
    "        with open(MRRELFILE, \"r\", encoding=\"utf8\") as MRRELRAW:\n",
    "            # C0851745|A31370316|SDUI|CHD|C0003466|A31264097|SDUI||R191307594||MDRKOR||||N||\n",
    "        \n",
    "            SY = []\n",
    "            REL_SNOMED = []\n",
    "            for y,linha in enumerate(MRRELRAW):\n",
    "                linhaSeparada =  linha.split(\"|\")\n",
    "                CUI1 = linhaSeparada[0]\n",
    "                REL  = linhaSeparada[3]\n",
    "                CUI2 = linhaSeparada[4]\n",
    "                RELA = linhaSeparada[7]\n",
    "                SAB  = linhaSeparada[10]\n",
    "                if CUI1 != CUI2:\n",
    "                    if REL == \"SY\":\n",
    "                        SY.append((y,CUI1,CUI2,SAB,REL,RELA))\n",
    "                    if SAB == \"SNOMEDCT_US\":\n",
    "                        REL_SNOMED.append((y,CUI1,CUI2,SAB,REL,RELA))\n",
    "        \n",
    "        return snomed,SY,REL_SNOMED\n",
    "    #https://www.ncbi.nlm.nih.gov/books/NBK9685/table/ch03.T.concept_names_and_sources_file_mr/\n",
    "    #https://www.ncbi.nlm.nih.gov/books/NBK9685/table/ch03.T.related_concepts_file_mrrel_rrf/\n",
    "    #https://www.nlm.nih.gov/research/umls/sourcereleasedocs/index.html    \n",
    "    #https://www.nlm.nih.gov/research/umls/knowledge_sources/metathesaurus/release/abbreviations.html\n",
    "    \n",
    "   \n",
    "    \n",
    "def processUmlsFirstTime(MRCONSOFILE,MRXW_PORFILE,MRRELFILE,p):\n",
    "    #import só para ter certeza\n",
    "    import os\n",
    "    import json\n",
    "    #se arquivo ja existe não rodar\n",
    "    arquivoUMLSnormalizada = 'UMLSnormalizadaMAPCLIN.txt'\n",
    "    if not os.path.exists(arquivoUMLSnormalizada):\n",
    "        u = UMLSclass(p)\n",
    "        UMLSfiles = (MRCONSOFILE,MRXW_PORFILE,MRRELFILE)\n",
    "        print(\"Abrindo UMLS\")\n",
    "        UMLS = u.openFiles(UMLSfiles)\n",
    "        #Wall time: 1min 3s\n",
    "        print(\"Pré-processando UMLS\")\n",
    "        UMLSNormalizada = u.selectUMLS(UMLS)\n",
    "        #Wall time: 16min 6s\n",
    "        del UMLS\n",
    "        print(\"Criando Stemm\")\n",
    "        UMLSNormalizadaSTEM = u.stemmUMLS(UMLSNormalizada)\n",
    "        #Wall time: 3min 17s\n",
    "        del UMLSNormalizada\n",
    "        print(\"Criando Lemma (DEMORA VÁRIAS HORAS!)\")\n",
    "        UMLSNormalizadaLEMMA = u.lematizaUMLS(UMLSNormalizadaSTEM)\n",
    "        #Wall time: 10h 2min 11s\n",
    "        del UMLSNormalizadaSTEM\n",
    "        # salvar as 3 variaveis\n",
    "        print(\"Salvando\")\n",
    "        with open(arquivoUMLSnormalizada, 'w') as f:\n",
    "            f.write(json.dumps(UMLSNormalizadaLEMMA))\n",
    "    \n",
    "\n",
    "def createSCTfiles(MRCONSOFILE,MRRELFILE,p,UMLStoSNOMEDfiles=[\"snomed.txt\",\"SY.txt\",\"REL_SNOMED.txt\"]):\n",
    "    #import só para ter certeza\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    \n",
    "    #se os arquivos ja foram criados, não precisa rodar esta função\n",
    "    if os.path.exists(UMLStoSNOMEDfiles[-1]):\n",
    "        return None\n",
    "    \n",
    "    u = UMLSclass(p)\n",
    "    snomed,SY,REL_SNOMED = u.openFilesUMLStoSNOMED(MRCONSOFILE,MRRELFILE)\n",
    "    #Wall time: 1min 57s\n",
    "    \n",
    "    with open(UMLStoSNOMEDfiles[0], 'w') as f:\n",
    "        f.write(json.dumps(snomed))\n",
    "    with open(UMLStoSNOMEDfiles[1], 'w') as f:\n",
    "        f.write(json.dumps(SY))\n",
    "    with open(UMLStoSNOMEDfiles[2], 'w') as f:\n",
    "        f.write(json.dumps(REL_SNOMED))\n",
    "\n",
    "        \n",
    "#Singleton    \n",
    "class OpenSCTfilesClass:\n",
    "    __instance = None\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def getInstance(cls,filesProcessados=[\"snomed.txt\",\"SY.txt\",\"REL_SNOMED.txt\"]):\n",
    "        if not cls.__instance:\n",
    "            cls.__instance = OpenSCTfilesClass()\n",
    "            cls.__instance.openSCTfiles(filesProcessados)\n",
    "        return cls.__instance\n",
    "\n",
    "    @classmethod       \n",
    "    def deleteInstance(cls):\n",
    "        cls.__instance = None\n",
    "        \n",
    "    def openSCTfiles(self,UMLStoSNOMEDfiles):\n",
    "        print(\"Abrindo arquivos SCT\")\n",
    "        \n",
    "        UMLStoSNOMEDdata = []\n",
    "        with open(UMLStoSNOMEDfiles[0], 'r') as f:\n",
    "            UMLStoSNOMEDdata.append(json.loads(f.read()))\n",
    "        with open(UMLStoSNOMEDfiles[1], 'r') as f:\n",
    "            UMLStoSNOMEDdata.append(json.loads(f.read()))\n",
    "        with open(UMLStoSNOMEDfiles[2], 'r') as f:\n",
    "            UMLStoSNOMEDdata.append(json.loads(f.read()))\n",
    "        #Wall time: 29.2 s\n",
    "        self.UMLStoSNOMEDdata = UMLStoSNOMEDdata\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening and Processing Intermediate Data and Creating Search Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#getitem é utilizado pela arvore\n",
    "_getitem0 = itemgetter(0)\n",
    "\n",
    "#https://github.com/benhoyt/pybktree\n",
    "class BKTree(object):\n",
    "    def __init__(self, distance_func, items=[]):\n",
    "        self.distance_func = distance_func\n",
    "        self.tree = None\n",
    "\n",
    "        _add = self.add\n",
    "        for item in items:\n",
    "            _add(item)\n",
    "\n",
    "    def add(self, item):\n",
    "        node = self.tree\n",
    "        if node is None:\n",
    "            self.tree = (item, {})\n",
    "            return\n",
    "\n",
    "        # Slight speed optimization -- avoid lookups inside the loop\n",
    "        _distance_func = self.distance_func\n",
    "\n",
    "        while True:\n",
    "            parent, children = node\n",
    "            distance = _distance_func(item, parent)\n",
    "            node = children.get(distance)\n",
    "            if node is None:\n",
    "                children[distance] = (item, {})\n",
    "                break\n",
    "\n",
    "    def find(self, item, n):\n",
    "        if self.tree is None:\n",
    "            return []\n",
    "\n",
    "        candidates = deque([self.tree])\n",
    "        found = []\n",
    "\n",
    "        # Slight speed optimization -- avoid lookups inside the loop\n",
    "        _candidates_popleft = candidates.popleft\n",
    "        _candidates_extend = candidates.extend\n",
    "        _found_append = found.append\n",
    "        _distance_func = self.distance_func\n",
    "\n",
    "        while candidates:\n",
    "            candidate, children = _candidates_popleft()\n",
    "            distance = _distance_func(candidate, item)\n",
    "            if distance <= n:\n",
    "                _found_append((distance, candidate))\n",
    "\n",
    "            if children:\n",
    "                lower = distance - n\n",
    "                upper = distance + n\n",
    "                _candidates_extend(c for d, c in children.items() if lower <= d <= upper)\n",
    "\n",
    "        found.sort(key=_getitem0)\n",
    "        return found\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.tree is None:\n",
    "            return\n",
    "\n",
    "        candidates = deque([self.tree])\n",
    "\n",
    "        # Slight speed optimization -- avoid lookups inside the loop\n",
    "        _candidates_popleft = candidates.popleft\n",
    "        _candidates_extend = candidates.extend\n",
    "\n",
    "        while candidates:\n",
    "            candidate, children = _candidates_popleft()\n",
    "            yield candidate\n",
    "            _candidates_extend(children.values())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<{} using {} with {} top-level nodes>'.format(\n",
    "            self.__class__.__name__,\n",
    "            self.distance_func.__name__,\n",
    "            len(self.tree[1]) if self.tree is not None else 'no',\n",
    "        )\n",
    "\n",
    "# wordsList = [\"help\",\"hell\",\"helps\",\"hello\",\"shell\",\"helper\",\"loop\",\"troop\"]\n",
    "# Item = collections.namedtuple('Item', ['id','word'])\n",
    "# def levenshtein_bk(x, y): #y is the searched word, so it doesnt need to be an Item\n",
    "#     return levenshtein(x.word, y)\n",
    "# \n",
    "# wordsItems = []\n",
    "# for idp,w in enumerate(wordsList):\n",
    "#     wordsItems.append(Item(idp,w))\n",
    "# \n",
    "# tree = BKTree(levenshtein_bk, wordsItems)\n",
    "# ##tree.add(\"hand\")              # add element\n",
    "# print(sorted(tree))            # BKTree instances are iterable\n",
    "# #['hell', 'hello', 'help', 'helper', 'helps', 'loop', 'shell', 'troop']\n",
    "# sorted(tree.find(\"oop\", 2))    # find elements at most n bit away from element x\n",
    "# #[(1, 'loop'), (2, 'troop')]\n",
    "    \n",
    "\n",
    "#funcoes soltas (necessarias)\n",
    "def destokeniza(texto):\n",
    "    a = \"\"\n",
    "    for token in texto:\n",
    "        a += \" \" + token.replace(\" \", \"-\")\n",
    "    return a[1:]\n",
    "Item = collections.namedtuple('Item', ['id','CUI','processadoTokenizado','processadoString','original','stemm','lemma'])\n",
    "def levenshtein_bk_processadoString(x, y):\n",
    "    return levenshtein(x.processadoString, y.processadoString)\n",
    "def levenshtein_bk_stemm(x, y): \n",
    "    return levenshtein(x.stemm, y.stemm)\n",
    "def levenshtein_bk_lemma(x, y):\n",
    "    return levenshtein(destokeniza(x.lemma), destokeniza(y.lemma))      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def createBKtree(arquivoUMLSnormalizada = 'UMLSnormalizadaMAPCLIN.txt',arquivosArvores = [\"BKtreeUMLSMRCONSO1.bktree\",\"BKtreeUMLSMRCONSO2.bktree\",\"BKtreeUMLSMRCONSO3.bktree\"]):\n",
    "    #import só para ter certeza\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    #se as arvores ja foram criadas, não precisa rodar esta função\n",
    "    if os.path.exists(arquivosArvores[-1]):\n",
    "        return None\n",
    "    \n",
    "    #se arquivo input não existe\n",
    "    if not os.path.exists(arquivoUMLSnormalizada):\n",
    "        print(\"Arquivo intermediario contendo UMLS normalizada não está no diretorio ou não existe (chamar função processUmlsFirstTime)\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Abrindo arquivo contendo UMLS normalizada\")\n",
    "    with open(arquivoUMLSnormalizada, 'r') as f:\n",
    "        UMLSNormalizadaLEMMA = json.loads(f.read())\n",
    "        \n",
    "    ##############################################################################\n",
    "    #se der erro no pickle cant use local object, remover essa parte    \n",
    "    def destokeniza(texto):\n",
    "        a = \"\"\n",
    "        for token in texto:\n",
    "            a += \" \" + token.replace(\" \", \"-\")\n",
    "        return a[1:]\n",
    "    \n",
    "    Item = collections.namedtuple('Item', ['id','CUI','processadoTokenizado','processadoString','original','stemm','lemma'])\n",
    "    def levenshtein_bk_processadoString(x, y):\n",
    "        return levenshtein(x.processadoString, y.processadoString)\n",
    "    def levenshtein_bk_stemm(x, y): \n",
    "        return levenshtein(x.stemm, y.stemm)\n",
    "    def levenshtein_bk_lemma(x, y):\n",
    "        return levenshtein(destokeniza(x.lemma), destokeniza(y.lemma))  \n",
    "    ##############################################################################\n",
    "    \n",
    "    \n",
    "    MRCONSO = UMLSNormalizadaLEMMA[0]\n",
    "    #MRXW_POR = UMLSNormalizadaLEMMA[1]\n",
    "    \n",
    "    #MRCONSO\n",
    "    MRCONSOitems = []\n",
    "    for data in MRCONSO:\n",
    "        MRCONSOitems.append(Item(data[0],data[1],data[2],data[3],data[4],data[5],data[6]))\n",
    "    print(\"Criando Arvore MRCONSO normal\")\n",
    "    treeMRCONSO1 = BKTree(levenshtein_bk_processadoString, MRCONSOitems)\n",
    "    fileObject = open(arquivosArvores[0], 'wb')\n",
    "    pickle.dump(treeMRCONSO1, fileObject)\n",
    "    del treeMRCONSO1\n",
    "    print(\"Criando Arvore MRCONSO stemm\")\n",
    "    treeMRCONSO2 = BKTree(levenshtein_bk_stemm, MRCONSOitems)\n",
    "    fileObject = open(arquivosArvores[1], 'wb')\n",
    "    pickle.dump(treeMRCONSO2, fileObject)\n",
    "    del treeMRCONSO2\n",
    "    print(\"Criando Arvore MRCONSO lemma\")\n",
    "    treeMRCONSO3 = BKTree(levenshtein_bk_lemma, MRCONSOitems)\n",
    "    fileObject = open(arquivosArvores[2], 'wb')\n",
    "    pickle.dump(treeMRCONSO3, fileObject)\n",
    "    del treeMRCONSO3\n",
    "    \n",
    "#Singleton    \n",
    "class OpenBKtreeClass:\n",
    "    __instance = None\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def getInstance(cls,filesProcessados=[\"BKtreeUMLSMRCONSO1.bktree\",\"BKtreeUMLSMRCONSO2.bktree\",\"BKtreeUMLSMRCONSO3.bktree\"]):\n",
    "        if not cls.__instance:\n",
    "            cls.__instance = OpenBKtreeClass()\n",
    "            cls.__instance.openBKtree(filesProcessados)\n",
    "        return cls.__instance\n",
    "\n",
    "    @classmethod    \n",
    "    def deleteInstance(cls):\n",
    "        cls.__instance = None\n",
    "    \n",
    "    def openBKtree(self,arquivosArvores):\n",
    "        print(\"Abrindo arvore UMLS\")\n",
    "        # abrir BK-tree\n",
    "        \n",
    "        treesUMLS = []\n",
    "        fileObject = open(arquivosArvores[0], 'rb')\n",
    "        treesUMLS.append(pickle.load(fileObject))\n",
    "        fileObject = open(arquivosArvores[1], 'rb')\n",
    "        treesUMLS.append(pickle.load(fileObject))\n",
    "        fileObject = open(arquivosArvores[2], 'rb')\n",
    "        treesUMLS.append(pickle.load(fileObject))\n",
    "        #Wall time: 43.7 s\n",
    "        self.treesUMLS = treesUMLS\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EstadoCodigoSCT:\n",
    "    def __init__(self):\n",
    "        self.ativoEspecificado = \"A-Conceito Ativo e Totalmente Especificado\"\n",
    "        self.ativo = \"B-Conceito Ativo\"\n",
    "        self.obsoleto = \"C-Conceito Obsoleto\"\n",
    "                \n",
    "class DataStructureOutputSearchTerm:\n",
    "    def __init__(self):\n",
    "        self.searchTerm = None\n",
    "        self.UMLSmatches = []\n",
    " \n",
    "    def toString(self):\n",
    "        if self.searchTerm == None:\n",
    "            return \"\"        \n",
    "        st = \"\"\n",
    "        st += \"Termo Buscado: \"+self.searchTerm+\"\\n\"\n",
    "        for UMLSmatch in self.UMLSmatches:\n",
    "            st += \"Possui relação com codigo UMLS:\\n\" + UMLSmatch.toString()   \n",
    "        return st     \n",
    "    \n",
    "    def toPandas(self):\n",
    "        df = pd.DataFrame(columns=['Termo Buscado',\n",
    "                           'CUI',\n",
    "                           'Termo UMLS',\n",
    "                           'Regra UMLS',\n",
    "                           'Codigo SCT',\n",
    "                           'Termo SCT',\n",
    "                           'Estado SCT',\n",
    "                           'Preferencia SCT'])\n",
    "\n",
    "        if len(self.UMLSmatches) == 0:\n",
    "            df = df.append({'Termo Buscado': self.searchTerm,\n",
    "                       'CUI':\"\",\n",
    "                       'Termo UMLS':\"\",\n",
    "                       'Regra UMLS':\"\",\n",
    "                       'Distancia Levenshtein':\"\",\n",
    "                       'Codigo SCT':\"\",\n",
    "                       'Termo SCT':\"\",\n",
    "                       'Estado SCT':\"\",\n",
    "                       'Preferencia SCT':\"\"}, ignore_index=True)\n",
    "            \n",
    "        for UMLSmatch in self.UMLSmatches:\n",
    "            if len(UMLSmatch.SCTmatches) == 0:\n",
    "                df = df.append({'Termo Buscado': self.searchTerm,\n",
    "                           'CUI':UMLSmatch.CUI,\n",
    "                           'Termo UMLS':UMLSmatch.original,\n",
    "                           'Regra UMLS':UMLSmatch.regraUMLS,\n",
    "                           'Distancia Levenshtein':UMLSmatch.distance,\n",
    "                           'Codigo SCT':\"\",\n",
    "                           'Termo SCT':\"\",\n",
    "                           'Estado SCT':\"\",\n",
    "                           'Preferencia SCT':\"\"}, ignore_index=True)\n",
    "                \n",
    "            for SCTmatch in UMLSmatch.SCTmatches:\n",
    "                df = df.append({'Termo Buscado': self.searchTerm,\n",
    "                           'CUI':UMLSmatch.CUI,\n",
    "                           'Termo UMLS':UMLSmatch.original,\n",
    "                           'Regra UMLS':UMLSmatch.regraUMLS,\n",
    "                           'Distancia Levenshtein':UMLSmatch.distance,\n",
    "                           'Codigo SCT':SCTmatch.CODESCT,\n",
    "                           'Termo SCT':SCTmatch.STR,\n",
    "                           'Estado SCT':SCTmatch.status,\n",
    "                           'Preferencia SCT':SCTmatch.ISPREF}, ignore_index=True)\n",
    "                \n",
    "        df = df.sort_values(by='Regra UMLS', ascending=True)\n",
    "        #mostrar somente primeira regra a ser usada\n",
    "        #if df.loc[0][\"Regra UMLS\"] == \"1-REGRA DIRETA\":\n",
    "        if \"1-REGRA DIRETA\" in df[\"Regra UMLS\"].tolist():\n",
    "            df.drop(df[df[\"Regra UMLS\"] != \"1-REGRA DIRETA\"].index, inplace=True)\n",
    "        if EstadoCodigoSCT().ativoEspecificado in df[\"Estado SCT\"].tolist():\n",
    "            df = df[df[\"Estado SCT\"] != EstadoCodigoSCT().obsoleto]\n",
    "            df = df[df[\"Estado SCT\"] != EstadoCodigoSCT().ativo]\n",
    "        elif EstadoCodigoSCT().ativo in df[\"Estado SCT\"].tolist():\n",
    "            df = df[df[\"Estado SCT\"] != EstadoCodigoSCT().obsoleto]\n",
    "        if \"Y\" in df[\"Preferencia SCT\"].tolist():    \n",
    "            df = df[df[\"Preferencia SCT\"] != \"N\"]\n",
    "        df.drop(['Preferencia SCT', 'Estado SCT'], axis = 1, inplace = True) \n",
    "        df = df.drop_duplicates(subset=['Termo Buscado','CUI', 'Termo UMLS', 'Codigo SCT', 'Termo SCT']) \n",
    "        return df\n",
    "    \n",
    "    def toPandasAll(self):\n",
    "        df = pd.DataFrame(columns=['Termo Buscado',\n",
    "                           'CUI',\n",
    "                           'Termo UMLS',\n",
    "                           'Regra UMLS',\n",
    "                           'Distancia Levenshtein',\n",
    "                           'Codigo SCT',\n",
    "                           'Termo SCT',\n",
    "                           'Estado SCT',\n",
    "                           'Preferencia SCT'])\n",
    "\n",
    "        if len(self.UMLSmatches) == 0:\n",
    "            df = df.append({'Termo Buscado': self.searchTerm,\n",
    "                       'CUI':\"\",\n",
    "                       'Termo UMLS':\"\",\n",
    "                       'Regra UMLS':\"\",\n",
    "                       'Distancia Levenshtein':\"\",\n",
    "                       'Codigo SCT':\"\",\n",
    "                       'Termo SCT':\"\",\n",
    "                       'Estado SCT':\"\",\n",
    "                       'Preferencia SCT':\"\",\n",
    "                        'Regra SCT':\"\"}, ignore_index=True)\n",
    "            \n",
    "        for UMLSmatch in self.UMLSmatches:\n",
    "            if len(UMLSmatch.SCTmatches) == 0:\n",
    "                df = df.append({'Termo Buscado': self.searchTerm,\n",
    "                           'CUI':UMLSmatch.CUI,\n",
    "                           'Termo UMLS':UMLSmatch.original,\n",
    "                           'Regra UMLS':UMLSmatch.regraUMLS,\n",
    "                           'Distancia Levenshtein':UMLSmatch.distance,\n",
    "                           'Codigo SCT':\"\",\n",
    "                           'Termo SCT':\"\",\n",
    "                           'Estado SCT':\"\",\n",
    "                           'Preferencia SCT':\"\",\n",
    "                           'Regra SCT':\"\"}, ignore_index=True)\n",
    "                \n",
    "            for SCTmatch in UMLSmatch.SCTmatches:\n",
    "                df = df.append({'Termo Buscado': self.searchTerm,\n",
    "                           'CUI':UMLSmatch.CUI,\n",
    "                           'Termo UMLS':UMLSmatch.original,\n",
    "                           'Regra UMLS':UMLSmatch.regraUMLS,\n",
    "                           'Distancia Levenshtein':UMLSmatch.distance,\n",
    "                           'Codigo SCT':SCTmatch.CODESCT,\n",
    "                           'Termo SCT':SCTmatch.STR,\n",
    "                           'Estado SCT':SCTmatch.status,\n",
    "                           'Preferencia SCT':SCTmatch.ISPREF,\n",
    "                           'Regra SCT':SCTmatch.regraSCT}, ignore_index=True)\n",
    "                \n",
    "        df = df.sort_values(by='Regra UMLS', ascending=True)\n",
    "        df = df.sort_values(by='Preferencia SCT', ascending=True)\n",
    "        df = df.sort_values(by='Estado SCT', ascending=True)\n",
    "        df = df.drop_duplicates(subset=['Termo Buscado','CUI', 'Termo UMLS', 'Codigo SCT', 'Termo SCT']) \n",
    "        return df\n",
    "        \n",
    "class DataStructureOutputUMLSmatch:\n",
    "    def __init__(self):\n",
    "        self.regraUMLS = None\n",
    "        self.distance = None\n",
    "        self.id = None\n",
    "        self.CUI = None\n",
    "        self.processadoTokenizado = None\n",
    "        self.processadoString = None\n",
    "        self.original = None\n",
    "        self.stemm = None\n",
    "        self.lemma = None\n",
    "        self.SCTmatches = []\n",
    "        \n",
    "    def toString(self):\n",
    "        if self.CUI == None:\n",
    "            return \"\"\n",
    "        st = \"\"\n",
    "        st += \"\\tCUI: \"+self.CUI+\"\\n\"\n",
    "        st += \"\\tTermo UMLS: \"+self.original+\"\\n\"\n",
    "        st += \"\\tObtido com: \"+self.regraUMLS+\"\\n\"\n",
    "        st += \"\\tDistancia: \"+str(self.distance)+\"\\n\"\n",
    "        for SCTmatch in self.SCTmatches:\n",
    "            st += \"\\tPossui relação com codigo SCT:\\n\" + SCTmatch.toString()\n",
    "        return st\n",
    "    \n",
    "class DataStructureOutputSCTmatch:\n",
    "    def __init__(self):\n",
    "        self.y = None     \n",
    "        self.regraSCT = None \n",
    "        self.CODESCT = None     \n",
    "        self.STR = None     \n",
    "        self.TTY = None      \n",
    "        self.status = None     \n",
    "        self.ISPREF = None     \n",
    "        self.STT = None #talvez deletar      \n",
    "        self.matchedCUI = None     \n",
    "        self.RELA = None     \n",
    "        \n",
    "    def toString(self):\n",
    "        if self.CODESCT == None:\n",
    "            return \"\"\n",
    "        st = \"\"\n",
    "        st += \"\\t\\tCodigo: \"+self.CODESCT+\"\\n\"\n",
    "        st += \"\\t\\tTermo SCT: \"+self.STR+\"\\n\"\n",
    "        st += \"\\t\\tStatus: \"+self.status+\" pois possui TTY:\"+self.TTY+\"\\n\"\n",
    "        st += \"\\t\\tPreferência: \"+self.ISPREF+\"\\n\"\n",
    "        st += \"\\t\\tMatch com CUI: \"+self.matchedCUI+\"\\n\"\n",
    "        if self.RELA != None:\n",
    "            st += \"\\t\\tRelação: \"+self.RELA+\"\\n\"\n",
    "        if self.regraSCT != None:\n",
    "            st += \"\\t\\tObtido com: \"+self.regraSCT+\"\\n\"\n",
    "        return st\n",
    "        \n",
    "\n",
    "class TemposExecucao:\n",
    "    def __init__(self):\n",
    "        self.R1 = None\n",
    "        self.R2 = None\n",
    "        self.R3 = None\n",
    "        self.R4 = None\n",
    "        self.R5 = None\n",
    "        self.R6 = None\n",
    "        self.R7 = None\n",
    "        self.R8 = None\n",
    "        self.BUSCA = None\n",
    "        self.C12 = None\n",
    "        self.C3 = None\n",
    "        self.C4 = None\n",
    "    \n",
    "    def criaEstatistica(self,listaTempos):\n",
    "        tempo = datetime.datetime.now()\n",
    "        tempo = tempo - tempo\n",
    "        soma = TemposExecucao()       \n",
    "        soma.R1 = tempo\n",
    "        soma.R2 = tempo\n",
    "        soma.R3 = tempo\n",
    "        soma.R4 = tempo\n",
    "        soma.R5 = tempo\n",
    "        soma.R6 = tempo\n",
    "        soma.R7 = tempo\n",
    "        soma.R8 = tempo\n",
    "        soma.BUSCA = tempo\n",
    "        soma.C12 = tempo\n",
    "        soma.C3 = tempo\n",
    "        soma.C4 = tempo \n",
    "        quantidadeUso = TemposExecucao()\n",
    "        quantidadeUso.R1 = 0\n",
    "        quantidadeUso.R2 = 0\n",
    "        quantidadeUso.R3 = 0\n",
    "        quantidadeUso.R4 = 0\n",
    "        quantidadeUso.R5 = 0\n",
    "        quantidadeUso.R6 = 0\n",
    "        quantidadeUso.R7 = 0\n",
    "        quantidadeUso.R8 = 0\n",
    "        quantidadeUso.BUSCA = 0\n",
    "        quantidadeUso.C12 = 0\n",
    "        quantidadeUso.C3 = 0\n",
    "        quantidadeUso.C4 = 0\n",
    "        for tempos in listaTempos:\n",
    "            if tempos.R1 != None:\n",
    "                soma.R1 += tempos.R1\n",
    "                quantidadeUso.R1 += 1\n",
    "            if tempos.R2 != None:\n",
    "                soma.R2 += tempos.R2\n",
    "                quantidadeUso.R2 += 1\n",
    "            if tempos.R3 != None:\n",
    "                soma.R3 += tempos.R3\n",
    "                quantidadeUso.R3 += 1\n",
    "            if tempos.R4 != None:\n",
    "                soma.R4 += tempos.R4\n",
    "                quantidadeUso.R4 += 1\n",
    "            if tempos.R5 != None:\n",
    "                soma.R5 += tempos.R5\n",
    "                quantidadeUso.R5 += 1\n",
    "            if tempos.R6 != None:\n",
    "                soma.R6 += tempos.R6\n",
    "                quantidadeUso.R6 += 1\n",
    "            if tempos.R7 != None:\n",
    "                soma.R7 += tempos.R7\n",
    "                quantidadeUso.R7 += 1\n",
    "            if tempos.R8 != None:\n",
    "                soma.R8 += tempos.R8\n",
    "                quantidadeUso.R8 += 1\n",
    "            if tempos.BUSCA != None:\n",
    "                soma.BUSCA += tempos.BUSCA\n",
    "                quantidadeUso.BUSCA += 1\n",
    "            if tempos.C12 != None:\n",
    "                soma.C12 += tempos.C12\n",
    "                quantidadeUso.C12 += 1\n",
    "            if tempos.C3 != None:\n",
    "                soma.C3 += tempos.C3\n",
    "                quantidadeUso.C3 += 1\n",
    "            if tempos.C4 != None:\n",
    "                soma.C4 += tempos.C4\n",
    "                quantidadeUso.C4 += 1\n",
    "      \n",
    "        media = TemposExecucao()  \n",
    "        if quantidadeUso.R1 != 0:\n",
    "            media.R1 =    ( soma.R1.total_seconds()/   quantidadeUso.R1    )\n",
    "        if quantidadeUso.R2 != 0:\n",
    "            media.R2 =    ( soma.R2.total_seconds()/   quantidadeUso.R2    )\n",
    "        if quantidadeUso.R3 != 0:\n",
    "            media.R3 =    ( soma.R3.total_seconds()/   quantidadeUso.R3    ) \n",
    "        if quantidadeUso.R4 != 0:\n",
    "            media.R4 =    ( soma.R4.total_seconds()/   quantidadeUso.R4    ) \n",
    "        if quantidadeUso.R5 != 0:\n",
    "            media.R5 =    ( soma.R5.total_seconds()/   quantidadeUso.R5    ) \n",
    "        if quantidadeUso.R6 != 0:\n",
    "            media.R6 =    ( soma.R6.total_seconds()/   quantidadeUso.R6    ) \n",
    "        if quantidadeUso.R7 != 0:\n",
    "            media.R7 =    ( soma.R7.total_seconds()/   quantidadeUso.R7    ) \n",
    "        if quantidadeUso.R8 != 0:\n",
    "            media.R8 =    ( soma.R8.total_seconds()/   quantidadeUso.R8    ) \n",
    "        if quantidadeUso.BUSCA != 0:\n",
    "            media.BUSCA = ( soma.BUSCA.total_seconds()/quantidadeUso.BUSCA ) \n",
    "        if quantidadeUso.C12   != 0:\n",
    "            media.C12 =   ( soma.C12.total_seconds()/  quantidadeUso.C12   ) \n",
    "        if quantidadeUso.C3    != 0:\n",
    "            media.C3 =    ( soma.C3.total_seconds()/   quantidadeUso.C3    ) \n",
    "        if quantidadeUso.C4    != 0:\n",
    "            media.C4 =    ( soma.C4.total_seconds()/   quantidadeUso.C4    ) \n",
    "        \n",
    "        return quantidadeUso,media,soma   \n",
    "                \n",
    "class OutputHelper:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def selectVarUsingVarWithCondition(self,output,varBusca,conditionFunction):\n",
    "        self.selectHolder = []\n",
    "        self.svuvwc(output,varBusca,conditionFunction)\n",
    "        return self.selectHolder\n",
    "        \n",
    "    def svuvwc(self,output,varBusca,conditionFunction): \n",
    "        #return selection of outputs\n",
    "        #Para realizar busca de 2 variaves que possuem mesma condição ao mesmo tempo, é possivel chamar 2 vezes essa função\n",
    "        if type(output) == list: #é lista\n",
    "            for out in output:\n",
    "                self.svuvwc(out,varBusca,conditionFunction)\n",
    "        elif type(output) != dict and type(output) != tuple and type(output) != str and type(output) != list and type(output) != float and type(output) != int: #é classe\n",
    "            if varBusca in list(vars(output).keys()):\n",
    "                exec(\"self.dadoVar = output.\"+varBusca)\n",
    "                if conditionFunction(self.dadoVar):\n",
    "                    self.selectHolder.append(self.dadoVar)\n",
    "            else: \n",
    "                #busca variavel que é lista\n",
    "                for item in list(vars(output).items()):\n",
    "                    if type(item[1]) == list:\n",
    "                        self.svuvwc(item[1],varBusca,conditionFunction)\n",
    "                        \n",
    "    def selectClassUsingVarWithCondition(self,output,varBusca,conditionFunction):\n",
    "        self.selectHolder = []\n",
    "        self.scusvwc(output,varBusca,conditionFunction)\n",
    "        return self.selectHolder\n",
    "            \n",
    "    \n",
    "    def scusvwc(self,output,varBusca,conditionFunction): \n",
    "        #return selection of outputs\n",
    "        #Para realizar busca de 2 variaves que possuem mesma condição ao mesmo tempo, é possivel chamar 2 vezes essa função\n",
    "        if type(output) == list: #é lista\n",
    "            for out in output:\n",
    "                self.scusvwc(out,varBusca,conditionFunction)\n",
    "        elif type(output) != dict and type(output) != tuple and type(output) != str and type(output) != list and type(output) != float and type(output) != int: #é classe\n",
    "            if varBusca in list(vars(output).keys()):\n",
    "                exec(\"self.dadoVar = output.\"+varBusca)\n",
    "                if conditionFunction(self.dadoVar):\n",
    "                    self.selectHolder.append(output)\n",
    "            else: \n",
    "                #busca variavel que é lista\n",
    "                for item in list(vars(output).items()):\n",
    "                    if type(item[1]) == list:\n",
    "                        self.scusvwc(item[1],varBusca,conditionFunction)\n",
    "    \n",
    "    \n",
    "    def selectAllClassUsingVarWithCondition(self,output,varBusca,conditionFunction):\n",
    "        self.selectHolder = []\n",
    "        for out in output:\n",
    "            if self.sacuvwc(out,varBusca,conditionFunction):\n",
    "                self.selectHolder.append(out)\n",
    "        return self.selectHolder\n",
    "            \n",
    "    \n",
    "    def sacuvwc(self,output,varBusca,conditionFunction): \n",
    "        #return selection of outputs\n",
    "        #Para realizar busca de 2 variaves que possuem mesma condição ao mesmo tempo, é possivel chamar 2 vezes essa função\n",
    "        if type(output) == list: #é lista\n",
    "            for out in output:\n",
    "                return self.sacuvwc(out,varBusca,conditionFunction)\n",
    "        elif type(output) != dict and type(output) != tuple and type(output) != str and type(output) != list and type(output) != float and type(output) != int: #é classe\n",
    "            if varBusca in list(vars(output).keys()):\n",
    "                exec(\"self.dadoVar = output.\"+varBusca)\n",
    "                if conditionFunction(self.dadoVar):\n",
    "                    return True\n",
    "            else: \n",
    "                #busca variavel que é lista\n",
    "                for item in list(vars(output).items()):\n",
    "                    if type(item[1]) == list:\n",
    "                        if self.sacuvwc(item[1],varBusca,conditionFunction):\n",
    "                            return True\n",
    "                        \n",
    "    def selectVar(self,output,var):\n",
    "        self.selectHolder = []\n",
    "        self.sv(output,var)\n",
    "        return self.selectHolder\n",
    "    \n",
    "    def sv(self,output,varBusca): \n",
    "        #return selection of outputs\n",
    "        #Para realizar busca de 2 variaves que possuem mesma condição ao mesmo tempo, é possivel chamar 2 vezes essa função\n",
    "        if type(output) == list: #é lista\n",
    "            for out in output:\n",
    "                self.sv(out,varBusca)\n",
    "        elif type(output) != dict and type(output) != tuple and type(output) != str and type(output) != list and type(output) != float and type(output) != int: #é classe\n",
    "            if varBusca in list(vars(output).keys()):\n",
    "                exec(\"self.dadoVar = output.\"+varBusca)\n",
    "                self.selectHolder.append(self.dadoVar)\n",
    "            else: \n",
    "                #busca variavel que é lista\n",
    "                for item in list(vars(output).items()):\n",
    "                    if type(item[1]) == list:\n",
    "                        self.sv(item[1],varBusca)    \n",
    "\n",
    "##                   \n",
    "#cada termo possui um lista de Matches UMLS e cada match UMLS possui uma lista de Matches SNOMED CT\n",
    "#vars(resultados[0])\n",
    "#vars(resultados[0].UMLSmatches[0])\n",
    "#vars(resultados[0].UMLSmatches[0].SCTmatches[0])\n",
    "#OutputHelper().selectVar(resultados,\"original\")[:10]\n",
    "#OutputHelper().selectVarUsingVarWithCondition(resultados,\"original\",lambda x: len(x) == 3)\n",
    "#OutputHelper().selectClassUsingVarWithCondition(resultados,\"original\",lambda x: len(x) == 3)\n",
    "#OutputHelper().selectAllClassUsingVarWithCondition(resultados,\"original\",lambda x: len(x) == 3)\n",
    "#caso = OutputHelper().selectAllClassUsingVarWithCondition(resultados,\"regraUMLS\",lambda x: x == \"1-REGRA DIRETA\")\n",
    "#print('REGRA DIRETA foi utilizada',len(caso),'vezes')\n",
    "#caso = OutputHelper().selectAllClassUsingVarWithCondition(resultados,\"regraUMLS\",lambda x: x == \"2-REGRA LEMMA\")\n",
    "#print('REGRA LEMMA foi utilizada',len(caso),'vezes')\n",
    "#caso = OutputHelper().selectAllClassUsingVarWithCondition(resultados,\"regraUMLS\",lambda x: x == \"3-REGRA STEMM\")\n",
    "#print('REGRA STEMM foi utilizada',len(caso),'vezes')\n",
    "#caso = OutputHelper().selectAllClassUsingVarWithCondition(resultados,\"regraUMLS\",lambda x: x == \"4-REGRA SINONIMO\")\n",
    "#print('REGRA SINONIMO foi utilizada',len(caso),'vezes')\n",
    "#caso = OutputHelper().selectAllClassUsingVarWithCondition(resultados,\"regraUMLS\",lambda x: x == \"5-REGRA PAI\")\n",
    "#print('REGRA PAI foi utilizada',len(caso),'vezes')\n",
    "#caso = OutputHelper().selectAllClassUsingVarWithCondition(resultados,\"regraUMLS\",lambda x: x == \"6-REGRA FILHO\")\n",
    "#print('REGRA FILHO foi utilizada',len(caso),'vezes')\n",
    "##Termos sem UMLS\n",
    "#casosSemUMLS = OutputHelper().selectAllClassUsingVarWithCondition(resultados,\"UMLSmatches\",lambda x: len(x) == 0)\n",
    "#print(len(casosSemUMLS),\"termos de busca não foram encontrados na UMLS\")\n",
    "#pandasDF = []\n",
    "#for resultado in casosSemUMLS:\n",
    "#    pandasDF.append(resultado.toPandas())\n",
    "#df = pd.concat(pandasDF)\n",
    "#df.reset_index(drop=True, inplace=True)\n",
    "#df.head(10)\n",
    "##Termos sem SCT\n",
    "#casosSemSCT = OutputHelper().selectAllClassUsingVarWithCondition(resultados,\"SCTmatches\",lambda x: len(x) == 0)\n",
    "#print(len(casosSemSCT),\"termos UMLS não foram encontrados na SNOMED CT\")\n",
    "#pandasDF = []\n",
    "#for resultado in casosSemSCT:\n",
    "#    pandasDF.append(resultado.toPandas())\n",
    "#df = pd.concat(pandasDF)\n",
    "#df.reset_index(drop=True, inplace=True)\n",
    "#df.head(10)\n",
    "#\n",
    "#def plot2Lists(l1,l2,title = \"\",ylabel = \"\", xlabel = \"\"):\n",
    "#    plt.title(title)\n",
    "#    plt.ylabel(ylabel)\n",
    "#    plt.xlabel(xlabel)\n",
    "#    plt.plot(l1,l2)\n",
    "#    plt.show()\n",
    "#def plotTermsSize(terms):\n",
    "#    sizeFrequencyDict = dict()\n",
    "#    for termo in terms:\n",
    "#        size = len(termo)\n",
    "#        try:\n",
    "#            sizeFrequencyDict[size] += 1 \n",
    "#        except:\n",
    "#            sizeFrequencyDict[size] = 1 \n",
    "#            \n",
    "#    sizeFrequency = [(k,v) for k, v in sizeFrequencyDict.items()]\n",
    "#    sizeFrequency.sort(key=lambda x: x[0])\n",
    "#    \n",
    "#    word_rank,word_frequency = zip(*sizeFrequency)\n",
    "#    plot2Lists(word_rank,word_frequency,title = \"Terms Size\",ylabel = \"Occurrences\", xlabel = \"Term Size\")\n",
    "#    \n",
    "#print(\"termos buscados\")\n",
    "#searchedTerms = OutputHelper().selectVar(resultados,\"searchTerm\")\n",
    "#plotTermsSize(searchedTerms)\n",
    "#print(\"termos UMLS encontrados\")\n",
    "#searchedTerms = OutputHelper().selectVar(resultados,\"original\")\n",
    "#plotTermsSize(searchedTerms)\n",
    "#print(\"termos SNOMED CT encontrados\")\n",
    "#searchedTerms = OutputHelper().selectVar(resultados,\"STR\")\n",
    "#plotTermsSize(searchedTerms)\n",
    "#\n",
    "#vars(temposExecucao[0].criaEstatistica(temposExecucao)[2])\n",
    "#vars(temposExecucao[0].criaEstatistica(temposExecucao)[1])\n",
    "#vars(temposExecucao[0].criaEstatistica(temposExecucao)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Class for UMLS and SNOMED CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MapUMLS:\n",
    "    def __init__(self,preProcessamentoclass):\n",
    "        self.preProcessamento = preProcessamentoclass\n",
    "    \n",
    "    def regraSinonimWordEmbedding(self,texto,arvoresUMLS,distanciaMaxima,WEC): \n",
    "        if distanciaMaxima != 0:\n",
    "            distanciaMaxima -= 1\n",
    "        \n",
    "        arvoreUMLS1 = arvoresUMLS[0]\n",
    "        arvoreUMLS2 = arvoresUMLS[1]\n",
    "        arvoreUMLS3 = arvoresUMLS[2]\n",
    "        \n",
    "        \n",
    "        conceitosCandidatosNAOIDENTICOS = []\n",
    "        if len(texto) == 0:\n",
    "            return []\n",
    "        else:\n",
    "            sinonimos = [tes[0] for tes in WEC.getSimilar(texto,topn=5)]\n",
    "            \n",
    "            if sinonimos == []:\n",
    "                return []\n",
    "            \n",
    "            for sinonimo in sinonimos:\n",
    "                sinonimo = self.preProcessamento.processaMAPCLIN(sinonimo)\n",
    "                _,conceitosEncontradosD = self.regraDiretaTree(sinonimo,arvoreUMLS1,distanciaMaxima)\n",
    "                conceitosEncontradosL = self.regraLemmaTree(self.preProcessamento.lematizador(sinonimo),arvoreUMLS3,distanciaMaxima)\n",
    "                conceitosEncontradosS = self.regraStemmTree(self.preProcessamento.stemmer(self.preProcessamento.destokeniza(sinonimo)),arvoreUMLS2,distanciaMaxima)\n",
    "                if len(conceitosEncontradosD) > 0:\n",
    "                    conceitosCandidatosNAOIDENTICOS.extend(conceitosEncontradosD)\n",
    "                if len(conceitosEncontradosL) > 0:\n",
    "                    conceitosCandidatosNAOIDENTICOS.extend(conceitosEncontradosL)\n",
    "                if len(conceitosEncontradosS) > 0:\n",
    "                    conceitosCandidatosNAOIDENTICOS.extend(conceitosEncontradosS)\n",
    "            return conceitosCandidatosNAOIDENTICOS    \n",
    "    \n",
    "    def regraSinonim(self,texto,arvoresUMLS,distanciaMaxima): \n",
    "        if distanciaMaxima != 0:\n",
    "            distanciaMaxima -= 1\n",
    "        \n",
    "        arvoreUMLS1 = arvoresUMLS[0]\n",
    "        arvoreUMLS2 = arvoresUMLS[1]\n",
    "        arvoreUMLS3 = arvoresUMLS[2]\n",
    "        \n",
    "        dicio = Dicio()\n",
    "        conceitosCandidatosNAOIDENTICOS = []\n",
    "        if len(texto) == 0:\n",
    "            return []\n",
    "        elif len(texto) == 1: #possui um unico token\n",
    "            tokenEncontrado = dicio.search(texto[0])\n",
    "            if tokenEncontrado == None:\n",
    "                return []\n",
    "            sinonimos = tokenEncontrado.synonyms\n",
    "            for sinonimo in sinonimos:\n",
    "                sinonimo = sinonimo.word\n",
    "                sinonimo = self.preProcessamento.processaMAPCLIN(sinonimo)\n",
    "                _,conceitosEncontradosD = self.regraDiretaTree(sinonimo,arvoreUMLS1,distanciaMaxima)\n",
    "                conceitosEncontradosL = self.regraLemmaTree(self.preProcessamento.lematizador(sinonimo),arvoreUMLS3,distanciaMaxima)\n",
    "                conceitosEncontradosS = self.regraStemmTree(self.preProcessamento.stemmer(self.preProcessamento.destokeniza(sinonimo)),arvoreUMLS2,distanciaMaxima)\n",
    "                if len(conceitosEncontradosD) > 0:\n",
    "                    conceitosCandidatosNAOIDENTICOS.extend(conceitosEncontradosD)\n",
    "                if len(conceitosEncontradosL) > 0:\n",
    "                    conceitosCandidatosNAOIDENTICOS.extend(conceitosEncontradosL)\n",
    "                if len(conceitosEncontradosS) > 0:\n",
    "                    conceitosCandidatosNAOIDENTICOS.extend(conceitosEncontradosS)\n",
    "            return conceitosCandidatosNAOIDENTICOS\n",
    "        else:\n",
    "            conceitosCtoken = []\n",
    "            for token in texto:\n",
    "                tokenEncontrado = dicio.search(texto[0])\n",
    "                if tokenEncontrado == None:\n",
    "                    continue\n",
    "                sinonimos = tokenEncontrado.synonyms\n",
    "                for sinonimo in sinonimos:\n",
    "                    sinonimo = sinonimo.word\n",
    "                    sinonimo = self.preProcessamento.processaMAPCLIN(sinonimo)\n",
    "                    _,conceitosEncontradosD = self.regraDiretaTree(sinonimo,arvoreUMLS1,distanciaMaxima)\n",
    "                    conceitosEncontradosL = self.regraLemmaTree(self.preProcessamento.lematizador(sinonimo),arvoreUMLS3,distanciaMaxima)\n",
    "                    conceitosEncontradosS = self.regraStemmTree(self.preProcessamento.stemmer(self.preProcessamento.destokeniza(sinonimo)),arvoreUMLS2,distanciaMaxima)\n",
    "                    if len(conceitosEncontradosD) > 0:\n",
    "                        conceitosCtoken.extend(tuple(conceitosEncontradosD))\n",
    "                    if len(conceitosEncontradosL) > 0:\n",
    "                        conceitosCtoken.extend(tuple(conceitosEncontradosL))\n",
    "                    if len(conceitosEncontradosS) > 0:\n",
    "                        conceitosCtoken.extend(tuple(conceitosEncontradosS))\n",
    "                        \n",
    "            for conceito in purge_dublicates(conceitosCtoken): \n",
    "                #se possui 1 palavra a mais ou a menos, ou igual\n",
    "                if abs(len(conceito[1].processadoTokenizado) - len(texto)) <= 1:\n",
    "                    conceitosCandidatosNAOIDENTICOS.append(conceito)\n",
    "            return conceitosCandidatosNAOIDENTICOS         \n",
    "                                    \n",
    "        \n",
    "    def regraPai(self,texto,arvoresUMLS,distanciaMaxima):        \n",
    "        arvoreUMLS1 = arvoresUMLS[0]\n",
    "        arvoreUMLS2 = arvoresUMLS[1]\n",
    "        arvoreUMLS3 = arvoresUMLS[2]\n",
    "        if len(texto) > 1:\n",
    "            conceitosCandidatosNAOIDENTICOS = []\n",
    "            conceitosCtoken = []\n",
    "            for token in texto:\n",
    "                token = [token]\n",
    "                _,conceitosEncontradosD = self.regraDiretaTree(token,arvoreUMLS1,distanciaMaxima)\n",
    "                conceitosEncontradosL = self.regraLemmaTree(token,arvoreUMLS3,distanciaMaxima)\n",
    "                conceitosEncontradosS = self.regraStemmTree(token,arvoreUMLS2,distanciaMaxima)\n",
    "                if len(conceitosEncontradosD) > 0:\n",
    "                    conceitosCtoken.extend(tuple(conceitosEncontradosD))\n",
    "                if len(conceitosEncontradosL) > 0:\n",
    "                    conceitosCtoken.extend(tuple(conceitosEncontradosL))\n",
    "                if len(conceitosEncontradosS) > 0:\n",
    "                    conceitosCtoken.extend(tuple(conceitosEncontradosS))\n",
    "                    \n",
    "            for conceito in purge_dublicates(conceitosCtoken): \n",
    "            #for conceito in set(conceitosCtoken):\n",
    "                #originalMapclin -> if (conceitosCtoken.count(conceito) >= 2 or len(conceito) == 1) and conceito not in conceitosCandidatosNAOIDENTICOS:\n",
    "                #conceitotokenized = self.preProcessamento.tokeniza(conceito[0])\n",
    "                conceitotokenized = conceito[1].processadoTokenizado\n",
    "                #if conceitosCtoken.count(conceito[0]) >= len(conceitotokenized) and conceito[0] not in conceitosCandidatosNAOIDENTICOS:\n",
    "                if (conceitosCtoken.count(conceito) >= 2 or len(conceitotokenized) == 1) and conceito not in conceitosCandidatosNAOIDENTICOS:\n",
    "                    conceitosCandidatosNAOIDENTICOS.append(conceito)          \n",
    "            return conceitosCandidatosNAOIDENTICOS         \n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "        \n",
    "    def regraFilho(self,texto,arvoresUMLS,distanciaMaxima):       \n",
    "        arvoreUMLS1 = arvoresUMLS[0]\n",
    "        arvoreUMLS2 = arvoresUMLS[1]\n",
    "        arvoreUMLS3 = arvoresUMLS[2]\n",
    "        \n",
    "        conceitosCandidatosNAOIDENTICOS = []\n",
    "        conceitosCtoken = []\n",
    "        for token in texto:\n",
    "            token = [token]\n",
    "            _,conceitosEncontradosD = self.regraDiretaTree(token,arvoreUMLS1,distanciaMaxima)\n",
    "            conceitosEncontradosL = self.regraLemmaTree(token,arvoreUMLS3,distanciaMaxima)\n",
    "            conceitosEncontradosS = self.regraStemmTree(token,arvoreUMLS2,distanciaMaxima)\n",
    "            if len(conceitosEncontradosD) > 0:\n",
    "                conceitosCtoken.extend(tuple(conceitosEncontradosD))\n",
    "            if len(conceitosEncontradosL) > 0:\n",
    "                conceitosCtoken.extend(tuple(conceitosEncontradosL))\n",
    "            if len(conceitosEncontradosS) > 0:\n",
    "                conceitosCtoken.extend(tuple(conceitosEncontradosS))\n",
    "                \n",
    "        \n",
    "        for conceito in purge_dublicates(conceitosCtoken): \n",
    "        #for conceito in set(conceitosCtoken):\n",
    "            #if (conceitosCtoken.count(conceito) >= 2 or len(conceito) == 1) and conceito not in conceitosCandidatosNAOIDENTICOS:\n",
    "            #conceitotokenized = self.preProcessamento.tokeniza(conceito[0])\n",
    "            conceitotokenized = conceito[1].processadoTokenizado\n",
    "            #if len(conceitotokenized) >= len(texto) and conceito[0] not in conceitosCandidatosNAOIDENTICOS:\n",
    "            \n",
    "            if len(conceitotokenized) > len(texto) and conceito not in conceitosCandidatosNAOIDENTICOS:\n",
    "                conceitosCandidatosNAOIDENTICOS.append(conceito)\n",
    "        \n",
    "        return conceitosCandidatosNAOIDENTICOS    \n",
    "        \n",
    "        \n",
    "    def regraDiretaTree(self,texto,arvoreUMLS,distanciaMaxima):\n",
    "        \n",
    "        textoD = self.preProcessamento.destokeniza(texto)\n",
    "        Item = collections.namedtuple('Item', ['processadoString'])  \n",
    "        itemTextoD = Item(textoD)\n",
    "            \n",
    "        termosSimilares = sorted(arvoreUMLS.find(itemTextoD, distanciaMaxima))\n",
    "        \n",
    "        #remove termos quando o primeiro charactere é diferente\n",
    "        termosSimilaresHOLDER = []\n",
    "        for termosimilar in termosSimilares:\n",
    "            try:\n",
    "                if termosimilar[1].processadoString[0].lower() == textoD[0].lower():\n",
    "                    termosSimilaresHOLDER.append(termosimilar)\n",
    "            except:\n",
    "                pass\n",
    "        termosSimilares = termosSimilaresHOLDER\n",
    "        del termosSimilaresHOLDER\n",
    "        \n",
    "        #se existe palavra com distancia 0\n",
    "        conceitosCandidatosIdenticos = []\n",
    "        for termosimilar in termosSimilares:\n",
    "            if termosimilar[0] == 0:\n",
    "                conceitosCandidatosIdenticos.append(termosimilar)\n",
    "                \n",
    "        if len(conceitosCandidatosIdenticos) == 0:\n",
    "            return False, termosSimilares\n",
    "        else:\n",
    "            return True, conceitosCandidatosIdenticos\n",
    "        \n",
    "    def regraLemmaTree(self,texto,arvoreUMLS,distanciaMaxima):\n",
    "        textoD = self.preProcessamento.destokeniza(texto)\n",
    "        Item = collections.namedtuple('Item', ['lemma'])  \n",
    "        itemTextoD = Item(textoD)\n",
    "            \n",
    "        termosSimilares = sorted(arvoreUMLS.find(itemTextoD, distanciaMaxima))\n",
    "        \n",
    "        return termosSimilares     \n",
    "        \n",
    "    def regraStemmTree(self,texto,arvoreUMLS,distanciaMaxima):\n",
    "        textoD = self.preProcessamento.destokeniza(texto)\n",
    "        Item = collections.namedtuple('Item', ['stemm'])  \n",
    "        itemTextoD = Item(textoD)\n",
    "            \n",
    "        termosSimilares = sorted(arvoreUMLS.find(itemTextoD, distanciaMaxima))\n",
    "        \n",
    "        return termosSimilares    \n",
    "    \n",
    "    \n",
    "    def regraNgram(self,texto,arvoresUMLS,distanciaMaxima):  \n",
    "        def allNGRAMFromList(term):\n",
    "            listOfTerms = []\n",
    "            tamanho = len(term)\n",
    "            for n in range(tamanho-1):\n",
    "                for g in range(tamanho-n):\n",
    "                    newTerm = []\n",
    "                    for i in range(g,g+n+1):\n",
    "                        token = term[i]\n",
    "                        newTerm.append(token)\n",
    "                    listOfTerms.append(newTerm)\n",
    "            return listOfTerms\n",
    "        \n",
    "        if distanciaMaxima != 0:\n",
    "            distanciaMaxima -= 1\n",
    "        \n",
    "        \n",
    "        if len(texto) < 2:\n",
    "            return []\n",
    "        else:\n",
    "            arvoreUMLS1 = arvoresUMLS[0]\n",
    "            arvoreUMLS2 = arvoresUMLS[1]\n",
    "            arvoreUMLS3 = arvoresUMLS[2]\n",
    "            conceitosCandidatosNAOIDENTICOS = []\n",
    "            ngrams = allNGRAMFromList(texto)[::-1]\n",
    "            for ngramterm in ngrams:\n",
    "                _,conceitosEncontradosD = self.regraDiretaTree(ngramterm,arvoreUMLS1,distanciaMaxima)\n",
    "                if len(conceitosEncontradosD) > 0:\n",
    "                    conceitosCandidatosNAOIDENTICOS.extend(conceitosEncontradosD)\n",
    "            return conceitosCandidatosNAOIDENTICOS    \n",
    "    \n",
    "    def createOutputUMLS(self,output,conceitoCandidato,regra):\n",
    "        outputUMLSmatch = DataStructureOutputUMLSmatch()\n",
    "        outputUMLSmatch.regraUMLS = regra\n",
    "        outputUMLSmatch.distance = conceitoCandidato[0]\n",
    "        outputUMLSmatch.id = conceitoCandidato[1].id\n",
    "        outputUMLSmatch.CUI = conceitoCandidato[1].CUI\n",
    "        outputUMLSmatch.processadoTokenizado = conceitoCandidato[1].processadoTokenizado\n",
    "        outputUMLSmatch.processadoString = conceitoCandidato[1].processadoString\n",
    "        outputUMLSmatch.original = conceitoCandidato[1].original\n",
    "        outputUMLSmatch.stemm = conceitoCandidato[1].stemm\n",
    "        outputUMLSmatch.lemma = conceitoCandidato[1].lemma\n",
    "        \n",
    "        output.UMLSmatches.append(outputUMLSmatch)\n",
    "        return output\n",
    "    \n",
    "    def mapToUMLStree(self,texto,arvoresUMLS,distanciaMaxima,output,temposExecucao,rules=[1,1,1,1,1,1,1,1],WEC=None):\n",
    "    \n",
    "        #distancia não pode ser maior que N, N = 80%\n",
    "        percSize = len(self.preProcessamento.destokeniza(texto))*0.20\n",
    "        if percSize < distanciaMaxima:\n",
    "            distanciaMaxima = int(percSize)\n",
    "        \n",
    "        arvoreUMLS1 = arvoresUMLS[0]\n",
    "        arvoreUMLS2 = arvoresUMLS[1]\n",
    "        arvoreUMLS3 = arvoresUMLS[2]\n",
    "        #arvoreUMLS4 = arvoresUMLS[3]\n",
    "        #arvoreUMLS5 = arvoresUMLS[4]\n",
    "        #arvoreUMLS6 = arvoresUMLS[5]\n",
    "        \n",
    "        if rules[0] == 1:\n",
    "            a = datetime.datetime.now()\n",
    "            isEqualMatch, conceitosCandidatos = self.regraDiretaTree(texto,arvoreUMLS1,distanciaMaxima)\n",
    "            if isEqualMatch: # se regra direta tiver distancia 0, deu match\n",
    "                for conceitoCandidato in conceitosCandidatos:\n",
    "                    output = self.createOutputUMLS(output,conceitoCandidato,\"1-REGRA DIRETA\")\n",
    "                b = datetime.datetime.now()\n",
    "                temposExecucao.R1 = b-a\n",
    "                return temposExecucao, output\n",
    "            \n",
    "            if len(conceitosCandidatos) > 0:\n",
    "                for conceitoCandidato in conceitosCandidatos:\n",
    "                    output = self.createOutputUMLS(output,conceitoCandidato,\"1-REGRA DIRETA\")\n",
    "                \n",
    "            b = datetime.datetime.now()\n",
    "            temposExecucao.R1 = b-a\n",
    "        \n",
    "    \n",
    "        if rules[1] == 1:\n",
    "            a = datetime.datetime.now()        \n",
    "            conceitosCandidatos = self.regraLemmaTree(texto,arvoreUMLS3,distanciaMaxima)\n",
    "            if len(conceitosCandidatos) > 0:\n",
    "                for conceitoCandidato in conceitosCandidatos:\n",
    "                    output = self.createOutputUMLS(output,conceitoCandidato,\"2-REGRA LEMMA\")\n",
    "                \n",
    "            b = datetime.datetime.now()\n",
    "            temposExecucao.R2 = b-a\n",
    "        \n",
    "        \n",
    "        if rules[2] == 1:\n",
    "            a = datetime.datetime.now()\n",
    "            conceitosCandidatos = self.regraStemmTree(texto,arvoreUMLS2,distanciaMaxima)\n",
    "            if len(conceitosCandidatos) > 0:\n",
    "                for conceitoCandidato in conceitosCandidatos:\n",
    "                    output = self.createOutputUMLS(output,conceitoCandidato,\"3-REGRA STEMM\")\n",
    "            b = datetime.datetime.now()\n",
    "            temposExecucao.R3 = b-a\n",
    "        \n",
    "        \n",
    "        if rules[3] == 1:\n",
    "            a = datetime.datetime.now()        \n",
    "            conceitosCandidatos = self.regraSinonim(texto,arvoresUMLS,distanciaMaxima)\n",
    "            if len(conceitosCandidatos) > 0:\n",
    "                for conceitoCandidato in conceitosCandidatos:\n",
    "                    output = self.createOutputUMLS(output,conceitoCandidato,\"4-REGRA SINONIMO\")\n",
    "            b = datetime.datetime.now()\n",
    "            temposExecucao.R4 = b-a\n",
    "\n",
    "        if rules[4] == 1:\n",
    "            a = datetime.datetime.now()            \n",
    "            conceitosCandidatos = self.regraPai(texto,arvoresUMLS,distanciaMaxima)\n",
    "            if len(conceitosCandidatos) > 0:\n",
    "                for conceitoCandidato in conceitosCandidatos:\n",
    "                    output = self.createOutputUMLS(output,conceitoCandidato,\"5-REGRA PAI\")\n",
    "            b = datetime.datetime.now()\n",
    "            temposExecucao.R5 = b-a\n",
    "            \n",
    "            \n",
    "        if rules[5] == 1:\n",
    "            a = datetime.datetime.now()\n",
    "            conceitosCandidatos = self.regraFilho(texto,arvoresUMLS,distanciaMaxima)\n",
    "            if len(conceitosCandidatos) > 0:\n",
    "                for conceitoCandidato in conceitosCandidatos:\n",
    "                    output = self.createOutputUMLS(output,conceitoCandidato,\"6-REGRA FILHO\")\n",
    "            b = datetime.datetime.now()\n",
    "            temposExecucao.R6 = b-a\n",
    "        \n",
    "        \n",
    "        if rules[6] == 1:\n",
    "            if WEC != None:\n",
    "                a = datetime.datetime.now()        \n",
    "                conceitosCandidatos = self.regraSinonimWordEmbedding(texto,arvoresUMLS,distanciaMaxima,WEC)\n",
    "                if len(conceitosCandidatos) > 0:\n",
    "                    for conceitoCandidato in conceitosCandidatos:\n",
    "                        output = self.createOutputUMLS(output,conceitoCandidato,\"7-REGRA SINONIMO WE\")\n",
    "                b = datetime.datetime.now()\n",
    "                temposExecucao.R7 = b-a\n",
    "\n",
    "        if rules[7] == 1:\n",
    "            a = datetime.datetime.now()        \n",
    "            conceitosCandidatos = self.regraNgram(texto,arvoresUMLS,distanciaMaxima)\n",
    "            if len(conceitosCandidatos) > 0:\n",
    "                for conceitoCandidato in conceitosCandidatos:\n",
    "                    output = self.createOutputUMLS(output,conceitoCandidato,\"8-REGRA NGRAM\")\n",
    "            b = datetime.datetime.now()\n",
    "            temposExecucao.R8 = b-a\n",
    "\n",
    "        \n",
    "                \n",
    "        \n",
    "        return temposExecucao,output\n",
    "    \n",
    "    \n",
    "    \n",
    "    def removeRepeticaoUMLS(self,output):\n",
    "        if len(output.UMLSmatches) < 2:\n",
    "            return output\n",
    "        i=0\n",
    "        for x in range(len(output.UMLSmatches)-1):\n",
    "            #se possui CUIS e String iguais\n",
    "            if output.UMLSmatches[i].CUI == output.UMLSmatches[i+1].CUI and output.UMLSmatches[i].processadoString == output.UMLSmatches[i+1].processadoString:\n",
    "                if int(output.UMLSmatches[i].regraUMLS[0]) > int(output.UMLSmatches[i+1].regraUMLS[0]):\n",
    "                    output.UMLSmatches.remove(output.UMLSmatches[i+1])\n",
    "                else:\n",
    "                    output.UMLSmatches.remove(output.UMLSmatches[i])\n",
    "                i=i-1\n",
    "            i=i+1\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MapSCT:     \n",
    "    def __init__(self,preProcessamentoclass):\n",
    "        self.preProcessamento = preProcessamentoclass\n",
    "    \n",
    "    def buscaSCT(self,output,snomed):\n",
    "        codigosSCT = []\n",
    "        for UMLSmatch in output.UMLSmatches:\n",
    "            for snomedTerm in snomed:\n",
    "                if snomedTerm[3] == UMLSmatch.CUI and snomedTerm not in codigosSCT:\n",
    "                    \n",
    "                    outputSCTmatch = DataStructureOutputSCTmatch()\n",
    "                    outputSCTmatch.y = snomedTerm[0]  \n",
    "                    outputSCTmatch.CODESCT = snomedTerm[1]     \n",
    "                    outputSCTmatch.STR = snomedTerm[2]            \n",
    "                    outputSCTmatch.matchedCUI = snomedTerm[3]   \n",
    "                    outputSCTmatch.TTY = snomedTerm[4] \n",
    "                    outputSCTmatch.ISPREF = snomedTerm[5]    \n",
    "                    outputSCTmatch.STT = snomedTerm[6] #talvez deletar \n",
    "                    \n",
    "                    UMLSmatch.SCTmatches.append(outputSCTmatch)\n",
    "                    \n",
    "                    codigosSCT.append(snomedTerm)\n",
    "                    \n",
    "        if len(codigosSCT) > 0:\n",
    "            return True, output\n",
    "        return False, output\n",
    "    \n",
    "    \n",
    "    def classificaStatus(self,output):\n",
    "        for UMLSmatch in output.UMLSmatches:\n",
    "            for outputSCTmatch in UMLSmatch.SCTmatches:\n",
    "                TTY = outputSCTmatch.TTY\n",
    "                if TTY == \"FN\":\n",
    "                    outputSCTmatch.status = EstadoCodigoSCT().ativoEspecificado\n",
    "                elif TTY == \"OF\" or TTY == \"OAS\" or TTY == \"OAP\" or TTY == \"IS\":\n",
    "                    outputSCTmatch.status = EstadoCodigoSCT().obsoleto\n",
    "                else:\n",
    "                    outputSCTmatch.status = EstadoCodigoSCT().ativo\n",
    "        return output\n",
    "    \n",
    "    def caso12(self,output):\n",
    "        for UMLSmatch in output.UMLSmatches:\n",
    "            for outputSCTmatch in UMLSmatch.SCTmatches:\n",
    "                outputSCTmatch.regraSCT = \"CASO12\"\n",
    "        return output\n",
    "    \n",
    "    def caso3(self,output,snomed,SY,REL_SNOMED):\n",
    "        codigosSCT = []\n",
    "        for UMLSmatch in output.UMLSmatches:\n",
    "            for outputSCTmatch in UMLSmatch.SCTmatches:\n",
    "                CUIumls = UMLSmatch.CUI\n",
    "                for rSY in SY:\n",
    "                    if rSY[1] == CUIumls:\n",
    "                        \n",
    "                        for snomedTerm in snomed:\n",
    "                            if snomedTerm[3] == rSY[2] and snomedTerm not in codigosSCT:\n",
    "                                TTY = snomedTerm[4]\n",
    "                                if TTY == \"FN\":         \n",
    "                                    outputSCTmatch.status = EstadoCodigoSCT().ativoEspecificado\n",
    "                                    outputSCTmatch.matchedCUI = outputSCTmatch.matchedCUI+\"+\"+rSY[2]\n",
    "                                    outputSCTmatch.RELA = \"SY\"       \n",
    "                                elif TTY == \"OF\" or TTY == \"OAS\" or TTY == \"OAP\" or TTY == \"IS\":       \n",
    "                                    outputSCTmatch.status = EstadoCodigoSCT().obsoleto\n",
    "                                    outputSCTmatch.matchedCUI = outputSCTmatch.matchedCUI+\"+\"+rSY[2]\n",
    "                                    outputSCTmatch.RELA = \"SY\"   \n",
    "                                else:      \n",
    "                                    outputSCTmatch.status = EstadoCodigoSCT().ativo\n",
    "                                    outputSCTmatch.matchedCUI = outputSCTmatch.matchedCUI+\"+\"+rSY[2]\n",
    "                                    outputSCTmatch.RELA = \"SY\"         \n",
    "                                codigosSCT.append(snomedTerm)\n",
    "                    elif rSY[2] == CUIumls:\n",
    "            \n",
    "                        for snomedTerm in snomed:\n",
    "                            if snomedTerm[3] == rSY[1] and snomedTerm not in codigosSCT:\n",
    "                                TTY = snomedTerm[4]\n",
    "                                if TTY == \"FN\":        \n",
    "                                    outputSCTmatch.status = EstadoCodigoSCT().ativoEspecificado\n",
    "                                    outputSCTmatch.matchedCUI = outputSCTmatch.matchedCUI+\"+\"+rSY[1]\n",
    "                                    outputSCTmatch.RELA = \"SY\"   \n",
    "                                elif TTY == \"OF\" or TTY == \"OAS\" or TTY == \"OAP\" or TTY == \"IS\":       \n",
    "                                    outputSCTmatch.status = EstadoCodigoSCT().obsoleto\n",
    "                                    outputSCTmatch.matchedCUI = outputSCTmatch.matchedCUI+\"+\"+rSY[1]\n",
    "                                    outputSCTmatch.RELA = \"SY\"   \n",
    "                                else:      \n",
    "                                    outputSCTmatch.status = EstadoCodigoSCT().ativo\n",
    "                                    outputSCTmatch.matchedCUI = outputSCTmatch.matchedCUI+\"+\"+rSY[1]\n",
    "                                    outputSCTmatch.RELA = \"SY\"           \n",
    "                                codigosSCT.append(snomedTerm)\n",
    "                for relsno in REL_SNOMED:\n",
    "                    rela = relsno[5]\n",
    "                    if rela == \"possibly_equivalent_to\" or rela == \"same_as\":\n",
    "                        if relsno[1] == CUIumls:\n",
    "                        \n",
    "                            for snomedTerm in snomed:\n",
    "                                if snomedTerm[3] == relsno[2] and snomedTerm not in codigosSCT:\n",
    "                                    TTY = snomedTerm[4]\n",
    "                                    if TTY == \"FN\":    \n",
    "                                        outputSCTmatch.status = EstadoCodigoSCT().ativoEspecificado\n",
    "                                        outputSCTmatch.matchedCUI = outputSCTmatch.matchedCUI+\"+\"+relsno[2]\n",
    "                                        outputSCTmatch.RELA = rela       \n",
    "                                    elif TTY == \"OF\" or TTY == \"OAS\" or TTY == \"OAP\" or TTY == \"IS\":       \n",
    "                                        outputSCTmatch.status = EstadoCodigoSCT().obsoleto\n",
    "                                        outputSCTmatch.matchedCUI = outputSCTmatch.matchedCUI+\"+\"+relsno[2]\n",
    "                                        outputSCTmatch.RELA = rela    \n",
    "                                    else:      \n",
    "                                        outputSCTmatch.status = EstadoCodigoSCT().ativo\n",
    "                                        outputSCTmatch.matchedCUI = outputSCTmatch.matchedCUI+\"+\"+relsno[2]\n",
    "                                        outputSCTmatch.RELA = rela          \n",
    "                                    codigosSCT.append(snomedTerm)\n",
    "                        elif relsno[2] == CUIumls:\n",
    "                \n",
    "                            for snomedTerm in snomed:\n",
    "                                if snomedTerm[3] == relsno[1] and snomedTerm not in codigosSCT:\n",
    "                                    TTY = snomedTerm[4]\n",
    "                                    if TTY == \"FN\":   \n",
    "                                        outputSCTmatch.status = EstadoCodigoSCT().ativoEspecificado\n",
    "                                        outputSCTmatch.matchedCUI = outputSCTmatch.matchedCUI+\"+\"+relsno[1]\n",
    "                                        outputSCTmatch.RELA = rela       \n",
    "                                    elif TTY == \"OF\" or TTY == \"OAS\" or TTY == \"OAP\" or TTY == \"IS\":       \n",
    "                                        outputSCTmatch.status = EstadoCodigoSCT().obsoleto\n",
    "                                        outputSCTmatch.matchedCUI = outputSCTmatch.matchedCUI+\"+\"+relsno[1]\n",
    "                                        outputSCTmatch.RELA = rela    \n",
    "                                    else:      \n",
    "                                        outputSCTmatch.status = EstadoCodigoSCT().ativo\n",
    "                                        outputSCTmatch.matchedCUI = outputSCTmatch.matchedCUI+\"+\"+relsno[1]\n",
    "                                        outputSCTmatch.RELA = rela              \n",
    "                                    codigosSCT.append(snomedTerm)\n",
    "                        \n",
    "        \n",
    "        for UMLSmatch in output.UMLSmatches:\n",
    "            for outputSCTmatch in UMLSmatch.SCTmatches:\n",
    "                outputSCTmatch.regraSCT = \"CASO3\"                \n",
    "                        \n",
    "        return output\n",
    "    \n",
    "    def caso4(self,output,snomed,SY):     \n",
    "        codigosSCT = []\n",
    "        for UMLSmatch in output.UMLSmatches:\n",
    "            CUIumls = UMLSmatch.CUI                \n",
    "            for rSY in SY:\n",
    "                if rSY[1] == CUIumls:\n",
    "                    for snomedTerm in snomed:\n",
    "                        if snomedTerm[3] == rSY[2] and snomedTerm not in codigosSCT:\n",
    "                            TTY = snomedTerm[4]\n",
    "                            outputSCTmatch = DataStructureOutputSCTmatch()\n",
    "                            \n",
    "                            if TTY == \"FN\":    \n",
    "                                outputSCTmatch.status = EstadoCodigoSCT().ativoEspecificado\n",
    "                            elif TTY == \"OF\" or TTY == \"OAS\" or TTY == \"OAP\" or TTY == \"IS\":       \n",
    "                                outputSCTmatch.status = EstadoCodigoSCT().obsoleto\n",
    "                            else:      \n",
    "                                outputSCTmatch.status = EstadoCodigoSCT().ativo\n",
    "                            \n",
    "                            outputSCTmatch.y = snomedTerm[0]  \n",
    "                            outputSCTmatch.CODESCT = snomedTerm[1]     \n",
    "                            outputSCTmatch.STR = snomedTerm[2]            \n",
    "                            outputSCTmatch.matchedCUI = snomedTerm[3] +\"+\"+rSY[2]   \n",
    "                            outputSCTmatch.TTY = snomedTerm[4] \n",
    "                            outputSCTmatch.ISPREF = snomedTerm[5]    \n",
    "                            outputSCTmatch.STT = snomedTerm[6] #talvez deletar    \n",
    "                            outputSCTmatch.RELA = \"SY\"\n",
    "                            outputSCTmatch.regraSCT = \"CASO 4\" \n",
    "                            \n",
    "                            UMLSmatch.SCTmatches.append(outputSCTmatch)\n",
    "                            \n",
    "                            codigosSCT.append(snomedTerm)\n",
    "                elif rSY[2] == CUIumls:\n",
    "                    for snomedTerm in snomed:\n",
    "                        if snomedTerm[3] == rSY[1] and snomedTerm not in codigosSCT:\n",
    "                            TTY = snomedTerm[4]\n",
    "                            outputSCTmatch = DataStructureOutputSCTmatch()\n",
    "                            \n",
    "                            if TTY == \"FN\":    \n",
    "                                outputSCTmatch.status = EstadoCodigoSCT().ativoEspecificado\n",
    "                            elif TTY == \"OF\" or TTY == \"OAS\" or TTY == \"OAP\" or TTY == \"IS\":       \n",
    "                                outputSCTmatch.status = EstadoCodigoSCT().obsoleto\n",
    "                            else:      \n",
    "                                outputSCTmatch.status = EstadoCodigoSCT().ativo\n",
    "                            \n",
    "                            outputSCTmatch.y = snomedTerm[0]  \n",
    "                            outputSCTmatch.CODESCT = snomedTerm[1]     \n",
    "                            outputSCTmatch.STR = snomedTerm[2]            \n",
    "                            outputSCTmatch.matchedCUI = snomedTerm[3] +\"+\"+rSY[1]   \n",
    "                            outputSCTmatch.TTY = snomedTerm[4] \n",
    "                            outputSCTmatch.ISPREF = snomedTerm[5]    \n",
    "                            outputSCTmatch.STT = snomedTerm[6] #talvez deletar    \n",
    "                            outputSCTmatch.RELA = \"SY\"\n",
    "                            outputSCTmatch.regraSCT = \"CASO 4\" \n",
    "                            \n",
    "                            UMLSmatch.SCTmatches.append(outputSCTmatch)\n",
    "                                \n",
    "                            codigosSCT.append(snomedTerm)\n",
    "            \n",
    "        return output\n",
    "    \n",
    "\n",
    "    def existeConceitoAtivo(self,output):\n",
    "        for UMLSmatch in output.UMLSmatches:\n",
    "            for outputSCTmatch in UMLSmatch.SCTmatches:\n",
    "                if outputSCTmatch.status == EstadoCodigoSCT().ativoEspecificado or outputSCTmatch.status == EstadoCodigoSCT().ativo:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def mapToSNOMEDCT(self,output,UMLStoSNOMEDdata,temposExecucao):\n",
    "        \n",
    "        if len(output.UMLSmatches) == 0:\n",
    "            return temposExecucao,output\n",
    "        \n",
    "        #MRCONSO -> y,CODE,STR,CUI,TTY,ISPREF,STT\n",
    "        #MRREL -> y,CUI1,CUI2,SAB,REL,RELA\n",
    "        snomed = UMLStoSNOMEDdata[0]\n",
    "        SY = UMLStoSNOMEDdata[1]\n",
    "        REL_SNOMED = UMLStoSNOMEDdata[2]\n",
    "        #REL_MTH = UMLStoSNOMEDdata[3]\n",
    "        \n",
    "        a = datetime.datetime.now()\n",
    "        encontrouCodigoSCT,output = self.buscaSCT(output,snomed)\n",
    "        b = datetime.datetime.now()\n",
    "        temposExecucao.BUSCA = b-a    \n",
    "            \n",
    "        if encontrouCodigoSCT: #se foi encontrado algum codigo SCT\n",
    "            a = datetime.datetime.now()\n",
    "            output = self.classificaStatus(output)\n",
    "            if self.existeConceitoAtivo(output):\n",
    "                output = self.caso12(output)\n",
    "                b = datetime.datetime.now()\n",
    "                temposExecucao.C12 = b-a   \n",
    "                return temposExecucao,output\n",
    "            else: #se todos os codigos são obsoletos\n",
    "                output = self.caso3(output,snomed,SY,REL_SNOMED)\n",
    "                #output pode não conter nenhum codigo ativo mesmo após caso3\n",
    "                b = datetime.datetime.now()\n",
    "                temposExecucao.C3 = b-a   \n",
    "                return temposExecucao,output\n",
    "            \n",
    "        else:\n",
    "            a = datetime.datetime.now()\n",
    "            #se não existe conceito SCT associado\n",
    "            output = self.caso4(output,snomed,SY)\n",
    "            b = datetime.datetime.now()\n",
    "            temposExecucao.C4 = b-a   \n",
    "            return temposExecucao,output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main (main function for searching terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mapMainUMLS(termos,filesUMLS=None,filesProcessados=None,rules=[1,0,0,0,0,0,0,1],distanciaMaxima=1,WEfile=None):\n",
    "    #INICIALIZANDO\n",
    "    p = PreProcessamento.getInstance()\n",
    "    m = MapUMLS(p)\n",
    "    \n",
    "    #Criando dados intermediarios e processados caso não existam\n",
    "    if filesUMLS != None:\n",
    "        MRCONSOFILE,MRXW_PORFILE,MRRELFILE = filesUMLS\n",
    "        \n",
    "        processUmlsFirstTime(MRCONSOFILE,MRXW_PORFILE,MRRELFILE,p)\n",
    "        if filesProcessados != None:\n",
    "            createBKtree(arquivosArvores=filesProcessados)\n",
    "        else:\n",
    "            createBKtree()\n",
    "            \n",
    "    #Abrindo Arvore de busca\n",
    "    if filesProcessados != None:\n",
    "        BKtreeClass = OpenBKtreeClass.getInstance(filesProcessados=filesProcessados)\n",
    "    else:\n",
    "        BKtreeClass = OpenBKtreeClass.getInstance()\n",
    "\n",
    "    WEC=None\n",
    "    if WEfile != None:\n",
    "        print(\"Abrindo Word Embedding\")\n",
    "        WEC = WordEmbeddingClass.getInstance(WEfile)\n",
    "        \n",
    "    #MAPEANDO\n",
    "    resultados = []\n",
    "    temposExecucao = []\n",
    "    position = 0\n",
    "    print(len(termos),\"termos\")\n",
    "    for termo in termos:\n",
    "        print(str(position),end=\"\\r\")\n",
    "        position+=1\n",
    "        \n",
    "        termoSProcessado = p.processaMAPCLINbusca(termo)\n",
    "    \n",
    "        output = DataStructureOutputSearchTerm()\n",
    "        output.searchTerm = termo\n",
    "        temposExecucaoz = TemposExecucao()\n",
    "    \n",
    "        for termoProcessado in termoSProcessado:\n",
    "    \n",
    "            if len(termoProcessado) != 0:\n",
    "                temposExecucaoz,output = m.mapToUMLStree(termoProcessado,\n",
    "                                                         BKtreeClass.treesUMLS,\n",
    "                                                         distanciaMaxima,\n",
    "                                                         output,\n",
    "                                                         temposExecucaoz,\n",
    "                                                         rules=rules,\n",
    "                                                         WEC=WEC)\n",
    "                output = m.removeRepeticaoUMLS(output)\n",
    "    \n",
    "        resultados.append(output)\n",
    "        temposExecucao.append(temposExecucaoz)\n",
    "\n",
    "    OpenBKtreeClass.deleteInstance()\n",
    "    if WEfile != None:\n",
    "        WEC.deleteInstance() \n",
    "    return temposExecucao,resultados\n",
    "   \n",
    "    \n",
    "    \n",
    "def mapMainSNOMEDCT(termos,outputs,tempo=None,filesUMLS=None,filesProcessados=None):\n",
    "    #INICIALIZANDO\n",
    "    p = PreProcessamento.getInstance()\n",
    "    m = MapSCT(p)\n",
    "    \n",
    "    #Criando dados intermediarios e processados caso não existam\n",
    "    if filesUMLS != None:\n",
    "        MRCONSOFILE,MRXW_PORFILE,MRRELFILE = filesUMLS\n",
    "        \n",
    "        processUmlsFirstTime(MRCONSOFILE,MRXW_PORFILE,MRRELFILE,p)\n",
    "        if filesProcessados != None:\n",
    "            createSCTfiles(MRCONSOFILE,MRRELFILE,p,UMLStoSNOMEDfiles=filesProcessados)\n",
    "        else:\n",
    "            createSCTfiles(MRCONSOFILE,MRRELFILE,p)\n",
    "            \n",
    "    #Abrindo Arvore de busca e dados de mapeamento para SNOMED CT\n",
    "    if filesProcessados != None:\n",
    "        SCTfileClass = OpenSCTfilesClass.getInstance(filesProcessados=filesProcessados)\n",
    "    else:\n",
    "        SCTfileClass = OpenSCTfilesClass.getInstance()    \n",
    "\n",
    "    #MAPEANDO\n",
    "    resultados = []\n",
    "    temposExecucao = []\n",
    "    position = 0\n",
    "    print(len(termos),\"termos\")\n",
    "    \n",
    "    if tempo == None:\n",
    "        tempo = [TemposExecucao() for _ in range(len(termos))]\n",
    "    for termo,output,temposExecucaoz in zip(termos,outputs,tempo):\n",
    "        print(str(position),end=\"\\r\")\n",
    "        position+=1\n",
    "        \n",
    "        termoSProcessado = p.processaMAPCLINbusca(termo)\n",
    "    \n",
    "        for termoProcessado in termoSProcessado:\n",
    "            \n",
    "            if len(termoProcessado) != 0:\n",
    "                temposExecucaoz,output = m.mapToSNOMEDCT(output,SCTfileClass.UMLStoSNOMEDdata,temposExecucaoz)\n",
    "            \n",
    "        temposExecucao.append(temposExecucaoz)\n",
    "        resultados.append(output)\n",
    "\n",
    "    OpenSCTfilesClass.deleteInstance()\n",
    "    return temposExecucao,resultados\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening Search Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#terms = [\"Cesáreas\",\"Paciente\",\"Edema\",\"Tacrolimo\",\"eletrocardiograma\",\"Retorno\",\"Curativo\",\"Seco\",\"Hemodiálise\",\n",
    "#          \"respiraçao espontanea\",\"dreno de suctor\",\"repouso no leito\",\"debito hematico\",\"debito sanguinolento\",\n",
    "#          \"acesso venoso periferico\",\"amarelo citrico\",\"incisao cirurgica abdominal\",\"evoluçao\",\"dia\",\"centro cirúrgico\"\n",
    "#          ,\"acordado\",\"responsivo\",\"esquerda\",\"dreno\",\"torax\",\"membro superior direito\",\"salinizado\",\"diurese\"]\n",
    "\n",
    "simpleTerms1 = [\"INTERNOU\",\"HIPOCORADO\",\"lesoes\",\"BIOPSIA\",\"dopamina\",\"dislipidemia\",\"vomitos\",\"atorvastatina\",\"anos\",\"familia\",\"normotenso\",\"sincope\",\"HX\",\"CILOSTAZOL\",\"Enfdeg\",\"MEDICO\",\"CBZ\",\"ESQUERDO\",\"FRALDA\",\"orientacao\",\"normocorado\",\"MEDICACAO\",\"ANLODIPINO\",\"Prescrevo\",\"palpitacoes\",\"POS-OP\",\"NORMOCORADA\",\"DEAMBULA\",\"MAOS\",\"Abdome\",\"selozok\",\"SEDADO\",\"fisioterapia\",\"DIPIRIDAMOL\",\"NOITE\",\"HOLTER\",\"digoxina\",\"QUEIXA-SE\",\"ENDO\",\"Abdomem\",\"tabagismo\",\"ORIENTACAO\",\"neurocirurgia\",\"SECA\",\"DEAMBULA\",\"INTERNOU\",\"PERIODO\",\"ticlopidina\",\"MPS\",\"ASMA\"]\n",
    "compoundTerms1 = [\"pupilas isocoricas\",\"IR\",\"HEMIMANDIBULECTOMIA DIREITA INTRA ORAL\",\"DTF\",\"mamas lactantes\",\"evacuacao ausente\",\"sucesso angiografico\",\"cateter venoso central duplo-lumen\",\"tto cx\",\"Rx torax\",\"dieta VO\",\"trach care\",\"RHA HIPOATIVOS\",\"EX FISICO\",\"NS FK\",\"PELE INTEGRA\",\"FAF\",\"cintilografia miocardica\",\"longa data\",\"rima labial\",\"reg sacra\",\"INDICACAO CIRURGICA\",\"IECA\",\"mucosa oral hidratada\",\"bx renal\",\"DOR PRECORDIAL\",\"TID\",\"CEC\",\"ra\",\"ABO\",\"ACEITANDO DIETA\",\"TRANSPLANTE RENAL\",\"SNE alimentacao\",\"angina instavel\",\"TOSSE PRODUTIVA\",\"Cx bariatrica\",\"modo PSV\",\"BOA PERFUSAO PERIFERICA\",\"secrecao traqueobronquica\",\"BOA ACEITACAO DA DIETA VO\",\"PARCIAL DE URINA\",\"PSAP\",\"RHA NORMOATIVOS\",\"IC ETIOLOGIA ISQUEMICA\",\"HVE\",\"loquios fisiologicos\",\"pos-op\",\"MONITORIZACAO CARDIACA\",\"Pi\",\"cuidados intensivos enfermagem\"]\n",
    "simpleTerms2 = [\"hipertensão\",\"feocroocitoma\",\"hipocalemia\",\"difuso\",\"hipocôndrio\",\"hipertenso\",\"noturno\",\"uterino\",\"sono\",\"reativa\",\"afebril\",\"aortico\",\"arterial\",\"ronco\",\"aterosclerose\",\"corada\",\"hiopoglicemia\",\"assintomática\",\"abdominal\",\"maços/dia\",\"depressão\",\"função\",\"edema\",\"piora\",\"história\",\"vícios\",\"obesidade\",\"sincope\",\"controlada\",\"dlp\",\"ex-tabagista\",\"dispneia\",\"tosse\",\"feocromocitoma\",\"hipotireioidismo\",\"proteinúria\",\"palpitações\",\"tontura\",\"dor precordial\",\"asma/dpoc\",\"cushing\",\"hasbled\",\"tromboembolico\"]\n",
    "compoundTerms2 = [\"apneia do sono\",\"aneurisma fusiforme infrarrenal\",\"dispneia aos moderados a grandes esforços\",\"obesidade centrípeta\",\"sopro protosistólico ejetivo com pico meso 3+/6+\",\"controles pressóricos\",\"fa persistente\",\"dlp mista\",\"dificil palpação\",\"alterações da repolarização ventricular\",\"has de provável etiologia secundária\",\"síndrome de qt longo induzido\",\"lesão renal\",\"drc - estagio 4\",\"boa aderência medicamentosa\",\"mv+ em aht s/ra\",\"sono de boa qualidade\",\"calcificação valvar\",\"sonolência diurna\",\"sopros carotídeos\",\"obesidade grau 2\",\"pressão elevados\",\"lesão renal prévia\",\"vd normal\",\"condução av variavel\",\"has secundária\",\"chadsvasc 2\",\"história patológica pregressa\",\"angina ortopneia\",\"etiologia não definida\",\"obesidade grau ii\",\"edema em mmii\",\"glicemia de jejum alterada\",\"descenso noturno\",\"facies cushingoide\",\"erisipelas de repetição\",\"lesoes troficas\",\"doença carotídea < 50 %\",\"etiologia multifatorial\",\"feocroocitoma operado\",\"função renal\",\"dor torácica\",\"dpn\",\"avc\",\"dm\",\"fc\",\"sve\",\"dm2\",\"drc\",\"has\",\"SAOS\",\"flutter atrial\",\"estenose de arteria renal à direita\",\"obesidade central\",\"has renovascular\",\"edema de mmii vespertino\",\"intolerante ieca\",\"cansaço aos moderados esforços\",\"edema de mmii 2+\"]\n",
    "\n",
    "TestSet1 = simpleTerms1+compoundTerms1\n",
    "TestSet2 = simpleTerms2+compoundTerms2\n",
    "AllTestSet = simpleTerms1+compoundTerms1+simpleTerms2+compoundTerms2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying UMLS Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abrindo arvore UMLS\n",
      "202 termos\n",
      "CPU times: total: 42.2 s\n",
      "Wall time: 41.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# As funções UMLS e SNOMED CT são chamadas em separado para diminuir a quantidade de arquivos em RAM e \n",
    "#para facilitar a adição de dados externos na extração dos dados da UMLS.\n",
    "\n",
    "filesProcessados = (\"BKtreeUMLSMRCONSO1.bktree\",\"BKtreeUMLSMRCONSO2.bktree\",\"BKtreeUMLSMRCONSO3.bktree\")\n",
    "\n",
    "#mask if you don't want to use any rule (direct, lemma, stemm, synonym, father, son, synonym WE)\n",
    "rules=[1,0,0,0,0,0,0,1]\n",
    "\n",
    "#WEfile =  'skip_s100.txt'\n",
    "#WEfile = 'word2vec-incor-2022.model'\n",
    "\n",
    "#temposExecucao,resultados = mapMainUMLS(AllTestSet,filesProcessados=filesProcessados,rules=rules,WEfile=WEfile)\n",
    "temposExecucao,resultados = mapMainUMLS(AllTestSet,filesProcessados=filesProcessados,rules=rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying SNOMED Only (requires UMLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abrindo arquivos SCT\n",
      "202 termos\n",
      "CPU times: total: 1min 30s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "filesProcessados = (\"snomed.txt\",\"SY.txt\",\"REL_SNOMED.txt\")\n",
    "\n",
    "temposExecucao,resultados = mapMainSNOMEDCT(AllTestSet,resultados,tempo=temposExecucao,filesProcessados=filesProcessados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving result terms in Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Termo Buscado</th>\n",
       "      <th>CUI</th>\n",
       "      <th>Termo UMLS</th>\n",
       "      <th>Regra UMLS</th>\n",
       "      <th>Codigo SCT</th>\n",
       "      <th>Termo SCT</th>\n",
       "      <th>Distancia Levenshtein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INTERNOU</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HIPOCORADO</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lesoes</td>\n",
       "      <td>C3263723</td>\n",
       "      <td>Lesões</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>417746004</td>\n",
       "      <td>Traumatic injury (disorder)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BIOPSIA</td>\n",
       "      <td>C0005558</td>\n",
       "      <td>Biopsia</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>129314006</td>\n",
       "      <td>Biopsy - action (qualifier value)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BIOPSIA</td>\n",
       "      <td>C0005558</td>\n",
       "      <td>Biopsia</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>86273004</td>\n",
       "      <td>Biopsy (procedure)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dopamina</td>\n",
       "      <td>C0013030</td>\n",
       "      <td>Dopamina</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>59187003</td>\n",
       "      <td>Product containing dopamine (medicinal product)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dopamina</td>\n",
       "      <td>C0013030</td>\n",
       "      <td>Dopamina</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>412383006</td>\n",
       "      <td>Dopamine (substance)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dislipidemia</td>\n",
       "      <td>C0242339</td>\n",
       "      <td>Dislipidemia</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>370992007</td>\n",
       "      <td>Dyslipidemia (disorder)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vomitos</td>\n",
       "      <td>C0042963</td>\n",
       "      <td>VOMITOS</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>300359004</td>\n",
       "      <td>Finding of vomiting (finding)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vomitos</td>\n",
       "      <td>C0042963</td>\n",
       "      <td>VOMITOS</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>249497008</td>\n",
       "      <td>Vomiting symptom (finding)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>vomitos</td>\n",
       "      <td>C0042963</td>\n",
       "      <td>VOMITOS</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>422400008</td>\n",
       "      <td>Vomiting (disorder)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>atorvastatina</td>\n",
       "      <td>C0286651</td>\n",
       "      <td>Atorvastatina</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>108600003</td>\n",
       "      <td>Product containing atorvastatin (medicinal pro...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>atorvastatina</td>\n",
       "      <td>C0286651</td>\n",
       "      <td>Atorvastatina</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>373444002</td>\n",
       "      <td>Atorvastatin (substance)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>anos</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>familia</td>\n",
       "      <td>C0015576</td>\n",
       "      <td>Família</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>35359004</td>\n",
       "      <td>Family (social concept)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>normotenso</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sincope</td>\n",
       "      <td>C0039070</td>\n",
       "      <td>SINCOPE</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>272030005</td>\n",
       "      <td>Syncope symptom (finding)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sincope</td>\n",
       "      <td>C0039070</td>\n",
       "      <td>SINCOPE</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>309585006</td>\n",
       "      <td>Syncope and collapse (disorder)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sincope</td>\n",
       "      <td>C0039070</td>\n",
       "      <td>SINCOPE</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>271594007</td>\n",
       "      <td>Syncope (disorder)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>HX</td>\n",
       "      <td>C0019621</td>\n",
       "      <td>Histiocitose X</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>190955000</td>\n",
       "      <td>Histiocytosis X syndrome (disorder)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>HX</td>\n",
       "      <td>C0019621</td>\n",
       "      <td>Histiocitose X</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>39795003</td>\n",
       "      <td>Hand-Schüller-Christian disease (disorder)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>HX</td>\n",
       "      <td>C0019621</td>\n",
       "      <td>Histiocitose X</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>65399007</td>\n",
       "      <td>Langerhans cell histiocytosis (disorder)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>HX</td>\n",
       "      <td>C0019621</td>\n",
       "      <td>Histiocitose X</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>128811003</td>\n",
       "      <td>Langerhans cell histiocytosis, multifocal (mor...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>HX</td>\n",
       "      <td>C0019621</td>\n",
       "      <td>Histiocitose X</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>128809007</td>\n",
       "      <td>Langerhans cell histiocytosis, no Internationa...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>CILOSTAZOL</td>\n",
       "      <td>C0055729</td>\n",
       "      <td>Cilostazol</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>395239000</td>\n",
       "      <td>Product containing cilostazol (medicinal product)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CILOSTAZOL</td>\n",
       "      <td>C0055729</td>\n",
       "      <td>Cilostazol</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>116087001</td>\n",
       "      <td>Cilostazol (substance)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Enfdeg</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MEDICO</td>\n",
       "      <td>C0031831</td>\n",
       "      <td>Médico</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>309343006</td>\n",
       "      <td>Physician (occupation)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>CBZ</td>\n",
       "      <td>C0006949</td>\n",
       "      <td>Carbamazepina</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>40820003</td>\n",
       "      <td>Product containing carbamazepine (medicinal pr...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>CBZ</td>\n",
       "      <td>C0006949</td>\n",
       "      <td>Carbamazepina</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>387222003</td>\n",
       "      <td>Carbamazepine (substance)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ESQUERDO</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>FRALDA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>orientacao</td>\n",
       "      <td>C0029266</td>\n",
       "      <td>Orientação</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>43173001</td>\n",
       "      <td>Orientation, function (observable entity)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>normocorado</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MEDICACAO</td>\n",
       "      <td>C0086597</td>\n",
       "      <td>Mediação</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>73510009</td>\n",
       "      <td>Mediate (qualifier value)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MEDICACAO</td>\n",
       "      <td>C0150277</td>\n",
       "      <td>Meditação</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ANLODIPINO</td>\n",
       "      <td>C0051696</td>\n",
       "      <td>Anlodipino</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>108537001</td>\n",
       "      <td>Product containing amlodipine (medicinal product)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ANLODIPINO</td>\n",
       "      <td>C0051696</td>\n",
       "      <td>Anlodipino</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>386864001</td>\n",
       "      <td>Amlodipine (substance)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Prescrevo</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>palpitacoes</td>\n",
       "      <td>C0030252</td>\n",
       "      <td>PALPITACOES</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>80313002</td>\n",
       "      <td>Palpitations (finding)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>POS-OP</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>NORMOCORADA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>DEAMBULA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>MAOS</td>\n",
       "      <td>C0018563</td>\n",
       "      <td>Mãos</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>85562004</td>\n",
       "      <td>Hand</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Abdome</td>\n",
       "      <td>C0000726</td>\n",
       "      <td>Abdome</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>818983003</td>\n",
       "      <td>Structure of abdominopelvic cavity and/or cont...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>selozok</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>SEDADO</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>fisioterapia</td>\n",
       "      <td>C0031818</td>\n",
       "      <td>Fisioterapia</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>fisioterapia</td>\n",
       "      <td>C0949766</td>\n",
       "      <td>Fisioterapia</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>91251008</td>\n",
       "      <td>Physical therapy procedure (regime/therapy)</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>DIPIRIDAMOL</td>\n",
       "      <td>C0012582</td>\n",
       "      <td>Dipiridamol</td>\n",
       "      <td>1-REGRA DIRETA</td>\n",
       "      <td>66859009</td>\n",
       "      <td>Product containing dipyridamole (medicinal pro...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Termo Buscado       CUI      Termo UMLS      Regra UMLS Codigo SCT  \\\n",
       "0        INTERNOU                                                        \n",
       "1      HIPOCORADO                                                        \n",
       "2          lesoes  C3263723          Lesões  1-REGRA DIRETA  417746004   \n",
       "3         BIOPSIA  C0005558         Biopsia  1-REGRA DIRETA  129314006   \n",
       "4         BIOPSIA  C0005558         Biopsia  1-REGRA DIRETA   86273004   \n",
       "5        dopamina  C0013030        Dopamina  1-REGRA DIRETA   59187003   \n",
       "6        dopamina  C0013030        Dopamina  1-REGRA DIRETA  412383006   \n",
       "7    dislipidemia  C0242339    Dislipidemia  1-REGRA DIRETA  370992007   \n",
       "8         vomitos  C0042963         VOMITOS  1-REGRA DIRETA  300359004   \n",
       "9         vomitos  C0042963         VOMITOS  1-REGRA DIRETA  249497008   \n",
       "10        vomitos  C0042963         VOMITOS  1-REGRA DIRETA  422400008   \n",
       "11  atorvastatina  C0286651   Atorvastatina  1-REGRA DIRETA  108600003   \n",
       "12  atorvastatina  C0286651   Atorvastatina  1-REGRA DIRETA  373444002   \n",
       "13           anos                                                        \n",
       "14        familia  C0015576         Família  1-REGRA DIRETA   35359004   \n",
       "15     normotenso                                                        \n",
       "16        sincope  C0039070         SINCOPE  1-REGRA DIRETA  272030005   \n",
       "17        sincope  C0039070         SINCOPE  1-REGRA DIRETA  309585006   \n",
       "18        sincope  C0039070         SINCOPE  1-REGRA DIRETA  271594007   \n",
       "19             HX  C0019621  Histiocitose X  1-REGRA DIRETA  190955000   \n",
       "20             HX  C0019621  Histiocitose X  1-REGRA DIRETA   39795003   \n",
       "21             HX  C0019621  Histiocitose X  1-REGRA DIRETA   65399007   \n",
       "22             HX  C0019621  Histiocitose X  1-REGRA DIRETA  128811003   \n",
       "23             HX  C0019621  Histiocitose X  1-REGRA DIRETA  128809007   \n",
       "24     CILOSTAZOL  C0055729      Cilostazol  1-REGRA DIRETA  395239000   \n",
       "25     CILOSTAZOL  C0055729      Cilostazol  1-REGRA DIRETA  116087001   \n",
       "26         Enfdeg                                                        \n",
       "27         MEDICO  C0031831          Médico  1-REGRA DIRETA  309343006   \n",
       "28            CBZ  C0006949   Carbamazepina  1-REGRA DIRETA   40820003   \n",
       "29            CBZ  C0006949   Carbamazepina  1-REGRA DIRETA  387222003   \n",
       "30       ESQUERDO                                                        \n",
       "31         FRALDA                                                        \n",
       "32     orientacao  C0029266      Orientação  1-REGRA DIRETA   43173001   \n",
       "33    normocorado                                                        \n",
       "34      MEDICACAO  C0086597        Mediação  1-REGRA DIRETA   73510009   \n",
       "35      MEDICACAO  C0150277       Meditação  1-REGRA DIRETA              \n",
       "36     ANLODIPINO  C0051696      Anlodipino  1-REGRA DIRETA  108537001   \n",
       "37     ANLODIPINO  C0051696      Anlodipino  1-REGRA DIRETA  386864001   \n",
       "38      Prescrevo                                                        \n",
       "39    palpitacoes  C0030252     PALPITACOES  1-REGRA DIRETA   80313002   \n",
       "40         POS-OP                                                        \n",
       "41    NORMOCORADA                                                        \n",
       "42       DEAMBULA                                                        \n",
       "43           MAOS  C0018563            Mãos  1-REGRA DIRETA   85562004   \n",
       "44         Abdome  C0000726          Abdome  1-REGRA DIRETA  818983003   \n",
       "45        selozok                                                        \n",
       "46         SEDADO                                                        \n",
       "47   fisioterapia  C0031818    Fisioterapia  1-REGRA DIRETA              \n",
       "48   fisioterapia  C0949766    Fisioterapia  1-REGRA DIRETA   91251008   \n",
       "49    DIPIRIDAMOL  C0012582     Dipiridamol  1-REGRA DIRETA   66859009   \n",
       "\n",
       "                                            Termo SCT Distancia Levenshtein  \n",
       "0                                                                            \n",
       "1                                                                            \n",
       "2                         Traumatic injury (disorder)                   0.0  \n",
       "3                   Biopsy - action (qualifier value)                   0.0  \n",
       "4                                  Biopsy (procedure)                   0.0  \n",
       "5     Product containing dopamine (medicinal product)                   0.0  \n",
       "6                                Dopamine (substance)                   0.0  \n",
       "7                             Dyslipidemia (disorder)                   0.0  \n",
       "8                       Finding of vomiting (finding)                   0.0  \n",
       "9                          Vomiting symptom (finding)                   0.0  \n",
       "10                                Vomiting (disorder)                   0.0  \n",
       "11  Product containing atorvastatin (medicinal pro...                   0.0  \n",
       "12                           Atorvastatin (substance)                   0.0  \n",
       "13                                                                           \n",
       "14                            Family (social concept)                   0.0  \n",
       "15                                                                           \n",
       "16                          Syncope symptom (finding)                   0.0  \n",
       "17                    Syncope and collapse (disorder)                   0.0  \n",
       "18                                 Syncope (disorder)                   0.0  \n",
       "19                Histiocytosis X syndrome (disorder)                   0.0  \n",
       "20         Hand-Schüller-Christian disease (disorder)                   0.0  \n",
       "21           Langerhans cell histiocytosis (disorder)                   0.0  \n",
       "22  Langerhans cell histiocytosis, multifocal (mor...                   0.0  \n",
       "23  Langerhans cell histiocytosis, no Internationa...                   0.0  \n",
       "24  Product containing cilostazol (medicinal product)                   0.0  \n",
       "25                             Cilostazol (substance)                   0.0  \n",
       "26                                                                           \n",
       "27                             Physician (occupation)                   0.0  \n",
       "28  Product containing carbamazepine (medicinal pr...                   0.0  \n",
       "29                          Carbamazepine (substance)                   0.0  \n",
       "30                                                                           \n",
       "31                                                                           \n",
       "32          Orientation, function (observable entity)                   0.0  \n",
       "33                                                                           \n",
       "34                          Mediate (qualifier value)                   1.0  \n",
       "35                                                                      1.0  \n",
       "36  Product containing amlodipine (medicinal product)                   0.0  \n",
       "37                             Amlodipine (substance)                   0.0  \n",
       "38                                                                           \n",
       "39                             Palpitations (finding)                   0.0  \n",
       "40                                                                           \n",
       "41                                                                           \n",
       "42                                                                           \n",
       "43                                               Hand                   0.0  \n",
       "44  Structure of abdominopelvic cavity and/or cont...                   0.0  \n",
       "45                                                                           \n",
       "46                                                                           \n",
       "47                                                                      0.0  \n",
       "48        Physical therapy procedure (regime/therapy)                   0.0  \n",
       "49  Product containing dipyridamole (medicinal pro...                   0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outFileName = \"outputMapClin2Exemplo.csv\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pandasDF = []\n",
    "for resultado in resultados:\n",
    "    pandasDF.append(resultado.toPandas())\n",
    "    #pandasDF.append(resultado.toPandasAll())\n",
    "df = pd.concat(pandasDF)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.to_csv(outFileName, encoding='utf-8-sig')\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(resultados[0].toString())\n",
    "#resultados[0].toPandas()\n",
    "#resultados[0].toPandasAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
